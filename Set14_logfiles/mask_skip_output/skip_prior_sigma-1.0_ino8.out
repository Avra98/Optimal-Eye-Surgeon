(3, 512, 512)
Starting vanilla DIP on 8 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '21.65047249623218'
(3, 512, 512) (3, 512, 512) torch.Size([1, 32, 512, 512])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/mask/8/skip/det/-1.0/1e-09
(3, 512, 512)
Starting vanilla DIP on 8 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '21.639319016290685'
(3, 512, 512) (3, 512, 512) torch.Size([1, 32, 512, 512])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/mask/8/skip/det/-1.0/1e-09
epoch:  0 quantization_loss:  0.17170113325119019
p mean is: tensor(-0.0002, device='cuda:1')
epoch:  1000 quantization_loss:  0.13720352947711945
p mean is: tensor(-0.0056, device='cuda:1')
epoch:  2000 quantization_loss:  0.13107116520404816
p mean is: tensor(-0.0094, device='cuda:1')
epoch:  3000 quantization_loss:  0.13390779495239258
p mean is: tensor(-0.0127, device='cuda:1')
epoch:  4000 quantization_loss:  0.13024432957172394
p mean is: tensor(-0.0157, device='cuda:1')
epoch:  5000 quantization_loss:  0.13193948566913605
p mean is: tensor(-0.0188, device='cuda:1')
epoch:  6000 quantization_loss:  0.1189868226647377
p mean is: tensor(-0.0224, device='cuda:1')
epoch:  7000 quantization_loss:  0.10656722635030746
p mean is: tensor(-0.0265, device='cuda:1')
epoch:  8000 quantization_loss:  0.09581918269395828
p mean is: tensor(-0.0319, device='cuda:1')
epoch:  9000 quantization_loss:  0.09445792436599731
p mean is: tensor(-0.0387, device='cuda:1')
epoch:  10000 quantization_loss:  0.09004906564950943
p mean is: tensor(-0.0474, device='cuda:1')
epoch:  11000 quantization_loss:  0.08861373364925385
p mean is: tensor(-0.0592, device='cuda:1')
epoch:  12000 quantization_loss:  0.08765362948179245
p mean is: tensor(-0.0739, device='cuda:1')
epoch:  13000 quantization_loss:  0.0840972363948822
p mean is: tensor(-0.0925, device='cuda:1')
epoch:  14000 quantization_loss:  0.08121838420629501
p mean is: tensor(-0.1152, device='cuda:1')
epoch:  15000 quantization_loss:  0.07628128677606583
p mean is: tensor(-0.1425, device='cuda:1')
epoch:  16000 quantization_loss:  0.07593852281570435
p mean is: tensor(-0.1735, device='cuda:1')
epoch:  17000 quantization_loss:  0.07446911185979843
p mean is: tensor(-0.2089, device='cuda:1')
epoch:  18000 quantization_loss:  0.07363054156303406
p mean is: tensor(-0.2483, device='cuda:1')
epoch:  19000 quantization_loss:  0.07349366694688797
p mean is: tensor(-0.2902, device='cuda:1')
epoch:  20000 quantization_loss:  0.07226401567459106
p mean is: tensor(-0.3334, device='cuda:1')
epoch:  21000 quantization_loss:  0.07215277850627899
p mean is: tensor(-0.3765, device='cuda:1')
epoch:  22000 quantization_loss:  0.07155447453260422
p mean is: tensor(-0.4186, device='cuda:1')
epoch:  23000 quantization_loss:  0.07144231349229813
p mean is: tensor(-0.4590, device='cuda:1')
epoch:  24000 quantization_loss:  0.07099774479866028
p mean is: tensor(-0.4969, device='cuda:1')
epoch:  25000 quantization_loss:  0.07077617943286896
p mean is: tensor(-0.5323, device='cuda:1')
epoch:  26000 quantization_loss:  0.07081777602434158
p mean is: tensor(-0.5650, device='cuda:1')
epoch:  27000 quantization_loss:  0.07051859050989151
p mean is: tensor(-0.5949, device='cuda:1')
epoch:  28000 quantization_loss:  0.07023291289806366
p mean is: tensor(-0.6222, device='cuda:1')
epoch:  29000 quantization_loss:  0.07003436237573624
p mean is: tensor(-0.6470, device='cuda:1')
epoch:  30000 quantization_loss:  0.0699341744184494
p mean is: tensor(-0.6690, device='cuda:1')
epoch:  31000 quantization_loss:  0.06980384886264801
p mean is: tensor(-0.6892, device='cuda:1')
epoch:  32000 quantization_loss:  0.06972789764404297
p mean is: tensor(-0.7073, device='cuda:1')
epoch:  33000 quantization_loss:  0.06963549554347992
p mean is: tensor(-0.7238, device='cuda:1')
epoch:  34000 quantization_loss:  0.0695124939084053
p mean is: tensor(-0.7386, device='cuda:1')
epoch:  35000 quantization_loss:  0.06941122561693192
p mean is: tensor(-0.7521, device='cuda:1')
epoch:  36000 quantization_loss:  0.06936760246753693
p mean is: tensor(-0.7646, device='cuda:1')
epoch:  37000 quantization_loss:  0.06932146102190018
p mean is: tensor(-0.7759, device='cuda:1')
epoch:  38000 quantization_loss:  0.06924349814653397
p mean is: tensor(-0.7862, device='cuda:1')
epoch:  39000 quantization_loss:  0.06917990744113922
p mean is: tensor(-0.7959, device='cuda:1')
epoch:  40000 quantization_loss:  0.06915988028049469
p mean is: tensor(-0.8049, device='cuda:1')
epoch:  41000 quantization_loss:  0.06911355257034302
p mean is: tensor(-0.8130, device='cuda:1')
epoch:  42000 quantization_loss:  0.06906826049089432
p mean is: tensor(-0.8205, device='cuda:1')
epoch:  43000 quantization_loss:  0.06901510059833527
p mean is: tensor(-0.8276, device='cuda:1')
epoch:  44000 quantization_loss:  0.06899002939462662
p mean is: tensor(-0.8343, device='cuda:1')
epoch:  45000 quantization_loss:  0.068971648812294
p mean is: tensor(-0.8404, device='cuda:1')
epoch:  46000 quantization_loss:  0.06893302500247955
p mean is: tensor(-0.8463, device='cuda:1')
epoch:  47000 quantization_loss:  0.06892042607069016
p mean is: tensor(-0.8519, device='cuda:1')
epoch:  48000 quantization_loss:  0.06888505816459656
p mean is: tensor(-0.8571, device='cuda:1')
epoch:  49000 quantization_loss:  0.06887128204107285
p mean is: tensor(-0.8621, device='cuda:1')
epoch:  50000 quantization_loss:  0.06886235624551773
p mean is: tensor(-0.8668, device='cuda:1')
epoch:  51000 quantization_loss:  0.06884368509054184
p mean is: tensor(-0.8711, device='cuda:1')
epoch:  52000 quantization_loss:  0.0688275620341301
p mean is: tensor(-0.8752, device='cuda:1')
epoch:  53000 quantization_loss:  0.06881657242774963
p mean is: tensor(-0.8793, device='cuda:1')
epoch:  54000 quantization_loss:  0.06880176067352295
p mean is: tensor(-0.8830, device='cuda:1')
epoch:  55000 quantization_loss:  0.06880199164152145
p mean is: tensor(-0.8867, device='cuda:1')
epoch:  56000 quantization_loss:  0.06878579407930374
p mean is: tensor(-0.8901, device='cuda:1')
epoch:  57000 quantization_loss:  0.06877179443836212
p mean is: tensor(-0.8934, device='cuda:1')
epoch:  58000 quantization_loss:  0.06876742094755173
p mean is: tensor(-0.8966, device='cuda:1')
epoch:  59000 quantization_loss:  0.06875777989625931
p mean is: tensor(-0.8997, device='cuda:1')
epoch:  60000 quantization_loss:  0.06874711811542511
p mean is: tensor(-0.9026, device='cuda:1')
epoch:  61000 quantization_loss:  0.06876741349697113
p mean is: tensor(-0.9055, device='cuda:1')
epoch:  62000 quantization_loss:  0.06872593611478806
p mean is: tensor(-0.9083, device='cuda:1')
epoch:  63000 quantization_loss:  0.06877771764993668
p mean is: tensor(-0.9109, device='cuda:1')
epoch:  64000 quantization_loss:  0.06871313601732254
p mean is: tensor(-0.9135, device='cuda:1')
epoch:  65000 quantization_loss:  0.06872210651636124
p mean is: tensor(-0.9160, device='cuda:1')
epoch:  66000 quantization_loss:  0.06869861483573914
p mean is: tensor(-0.9185, device='cuda:1')
epoch:  67000 quantization_loss:  0.0686989426612854
p mean is: tensor(-0.9209, device='cuda:1')
epoch:  68000 quantization_loss:  0.06869525462388992
p mean is: tensor(-0.9232, device='cuda:1')
epoch:  69000 quantization_loss:  0.0686848983168602
p mean is: tensor(-0.9255, device='cuda:1')
epoch:  70000 quantization_loss:  0.06867879629135132
p mean is: tensor(-0.9278, device='cuda:1')
epoch:  71000 quantization_loss:  0.06867668032646179
p mean is: tensor(-0.9300, device='cuda:1')
epoch:  72000 quantization_loss:  0.06867676228284836
p mean is: tensor(-0.9320, device='cuda:1')
epoch:  73000 quantization_loss:  0.06866249442100525
p mean is: tensor(-0.9340, device='cuda:1')
epoch:  74000 quantization_loss:  0.06866349279880524
p mean is: tensor(-0.9359, device='cuda:1')
epoch:  75000 quantization_loss:  0.06866320967674255
p mean is: tensor(-0.9378, device='cuda:1')
epoch:  76000 quantization_loss:  0.0686645582318306
p mean is: tensor(-0.9397, device='cuda:1')
epoch:  77000 quantization_loss:  0.0686524510383606
p mean is: tensor(-0.9416, device='cuda:1')
epoch:  78000 quantization_loss:  0.06865277886390686
p mean is: tensor(-0.9434, device='cuda:1')
epoch:  79000 quantization_loss:  0.06865154206752777
p mean is: tensor(-0.9452, device='cuda:1')
here
1.0.1.1.weight       | nonzeros =       5 /     128             (  3.91%) | total_pruned =     123 | shape = torch.Size([4, 32, 1, 1])
1.0.1.1.bias         | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.0.2.weight         | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.0.2.bias           | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.1.1.weight       | nonzeros =    1537 /    4608             ( 33.36%) | total_pruned =    3071 | shape = torch.Size([16, 32, 3, 3])
1.1.1.1.bias         | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.2.weight         | nonzeros =      11 /      16             ( 68.75%) | total_pruned =       5 | shape = torch.Size([16])
1.1.2.bias           | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.4.1.weight       | nonzeros =     566 /    2304             ( 24.57%) | total_pruned =    1738 | shape = torch.Size([16, 16, 3, 3])
1.1.4.1.bias         | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.5.weight         | nonzeros =      11 /      16             ( 68.75%) | total_pruned =       5 | shape = torch.Size([16])
1.1.5.bias           | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.7.1.0.1.1.weight | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([4, 16, 1, 1])
1.1.7.1.0.1.1.bias   | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.0.2.weight   | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.0.2.bias     | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.1.1.weight | nonzeros =    1349 /    4608             ( 29.28%) | total_pruned =    3259 | shape = torch.Size([32, 16, 3, 3])
1.1.7.1.1.1.1.bias   | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.2.weight   | nonzeros =      28 /      32             ( 87.50%) | total_pruned =       4 | shape = torch.Size([32])
1.1.7.1.1.2.bias     | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.4.1.weight | nonzeros =    2918 /    9216             ( 31.66%) | total_pruned =    6298 | shape = torch.Size([32, 32, 3, 3])
1.1.7.1.1.4.1.bias   | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.5.weight   | nonzeros =      27 /      32             ( 84.38%) | total_pruned =       5 | shape = torch.Size([32])
1.1.7.1.1.5.bias     | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.7.1.0.1.1.weight | nonzeros =      13 /     128             ( 10.16%) | total_pruned =     115 | shape = torch.Size([4, 32, 1, 1])
1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.0.2.weight | nonzeros =       1 /       4             ( 25.00%) | total_pruned =       3 | shape = torch.Size([4])
1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    5112 /   18432             ( 27.73%) | total_pruned =   13320 | shape = torch.Size([64, 32, 3, 3])
1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.2.weight | nonzeros =      56 /      64             ( 87.50%) | total_pruned =       8 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.4.1.weight | nonzeros =    7185 /   36864             ( 19.49%) | total_pruned =   29679 | shape = torch.Size([64, 64, 3, 3])
1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.5.weight | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.7.1.0.1.1.weight | nonzeros =      28 /     256             ( 10.94%) | total_pruned =     228 | shape = torch.Size([4, 64, 1, 1])
1.1.7.1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.0.2.weight | nonzeros =       1 /       4             ( 25.00%) | total_pruned =       3 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    7159 /   73728             (  9.71%) | total_pruned =   66569 | shape = torch.Size([128, 64, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.2.weight | nonzeros =     100 /     128             ( 78.12%) | total_pruned =      28 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.4.1.weight | nonzeros =   10301 /  147456             (  6.99%) | total_pruned =  137155 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.5.weight | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.weight | nonzeros =     201 /     512             ( 39.26%) | total_pruned =     311 | shape = torch.Size([4, 128, 1, 1])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.weight | nonzeros =       4 /       4             (100.00%) | total_pruned =       0 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    2878 /  147456             (  1.95%) | total_pruned =  144578 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.weight | nonzeros =      95 /     128             ( 74.22%) | total_pruned =      33 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.weight | nonzeros =    6002 /  147456             (  4.07%) | total_pruned =  141454 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.weight | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.weight | nonzeros =     108 /     512             ( 21.09%) | total_pruned =     404 | shape = torch.Size([4, 128, 1, 1])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.weight | nonzeros =       2 /       4             ( 50.00%) | total_pruned =       2 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.weight | nonzeros =   11274 /  147456             (  7.65%) | total_pruned =  136182 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.weight | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.weight | nonzeros =   22527 /  147456             ( 15.28%) | total_pruned =  124929 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.weight | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.2.weight | nonzeros =      88 /     132             ( 66.67%) | total_pruned =      44 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.2.bias | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.3.1.weight | nonzeros =   28591 /  152064             ( 18.80%) | total_pruned =  123473 | shape = torch.Size([128, 132, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.4.weight | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.2.weight | nonzeros =      88 /     132             ( 66.67%) | total_pruned =      44 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.2.bias | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.3.1.weight | nonzeros =   32727 /  152064             ( 21.52%) | total_pruned =  119337 | shape = torch.Size([128, 132, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.4.weight | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.2.weight | nonzeros =      93 /     132             ( 70.45%) | total_pruned =      39 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.2.bias | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.3.1.weight | nonzeros =   24765 /  152064             ( 16.29%) | total_pruned =  127299 | shape = torch.Size([128, 132, 3, 3])
1.1.7.1.1.7.1.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.4.weight | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.2.weight | nonzeros =      66 /     132             ( 50.00%) | total_pruned =      66 | shape = torch.Size([132])
1.1.7.1.1.7.2.bias   | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.3.1.weight | nonzeros =    8674 /   76032             ( 11.41%) | total_pruned =   67358 | shape = torch.Size([64, 132, 3, 3])
1.1.7.1.1.7.3.1.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.4.weight | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
1.1.7.1.1.7.4.bias   | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.2.weight       | nonzeros =      31 /      68             ( 45.59%) | total_pruned =      37 | shape = torch.Size([68])
1.1.7.2.bias         | nonzeros =       0 /      68             (  0.00%) | total_pruned =      68 | shape = torch.Size([68])
1.1.7.3.1.weight     | nonzeros =    2407 /   19584             ( 12.29%) | total_pruned =   17177 | shape = torch.Size([32, 68, 3, 3])
1.1.7.3.1.bias       | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.4.weight       | nonzeros =      18 /      32             ( 56.25%) | total_pruned =      14 | shape = torch.Size([32])
1.1.7.4.bias         | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
2.weight             | nonzeros =      18 /      36             ( 50.00%) | total_pruned =      18 | shape = torch.Size([36])
2.bias               | nonzeros =       0 /      36             (  0.00%) | total_pruned =      36 | shape = torch.Size([36])
3.1.weight           | nonzeros =    1197 /    5184             ( 23.09%) | total_pruned =    3987 | shape = torch.Size([16, 36, 3, 3])
3.1.bias             | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
4.weight             | nonzeros =      16 /      16             (100.00%) | total_pruned =       0 | shape = torch.Size([16])
4.bias               | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
6.1.weight           | nonzeros =      25 /      48             ( 52.08%) | total_pruned =      23 | shape = torch.Size([3, 16, 1, 1])
6.1.bias             | nonzeros =       1 /       3             ( 33.33%) | total_pruned =       2 | shape = torch.Size([3])
alive: 179011, pruned : 1272472, total: 1451483, Compression rate :       8.11x  ( 87.67% pruned)
PSNR of output image is:  11.248211266944676
Experiment done
