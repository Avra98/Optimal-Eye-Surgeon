(3, 256, 256)
Starting vanilla DIP on 3 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '20.342335954367723'
(3, 256, 256) (3, 256, 256) torch.Size([1, 32, 256, 256])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/mask/3/skip/det/-1.0/1e-09
(3, 256, 256)
Starting vanilla DIP on 3 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '20.306262203391007'
(3, 256, 256) (3, 256, 256) torch.Size([1, 32, 256, 256])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/mask/3/skip/det/-1.0/1e-09
epoch:  0 quantization_loss:  0.07338759303092957
p mean is: tensor(-0.0002, device='cuda:2')
epoch:  1000 quantization_loss:  0.06865211576223373
p mean is: tensor(-0.0058, device='cuda:2')
epoch:  2000 quantization_loss:  0.06916426867246628
p mean is: tensor(-0.0102, device='cuda:2')
epoch:  3000 quantization_loss:  0.0687955990433693
p mean is: tensor(-0.0143, device='cuda:2')
epoch:  4000 quantization_loss:  0.06911274045705795
p mean is: tensor(-0.0181, device='cuda:2')
epoch:  5000 quantization_loss:  0.06779284030199051
p mean is: tensor(-0.0225, device='cuda:2')
epoch:  6000 quantization_loss:  0.06861814111471176
p mean is: tensor(-0.0266, device='cuda:2')
epoch:  7000 quantization_loss:  0.06886539608240128
p mean is: tensor(-0.0309, device='cuda:2')
epoch:  8000 quantization_loss:  0.06838584691286087
p mean is: tensor(-0.0349, device='cuda:2')
epoch:  9000 quantization_loss:  0.06836060434579849
p mean is: tensor(-0.0389, device='cuda:2')
epoch:  10000 quantization_loss:  0.06866227090358734
p mean is: tensor(-0.0432, device='cuda:2')
epoch:  11000 quantization_loss:  0.0687398687005043
p mean is: tensor(-0.0474, device='cuda:2')
epoch:  12000 quantization_loss:  0.0682532861828804
p mean is: tensor(-0.0520, device='cuda:2')
epoch:  13000 quantization_loss:  0.06840038299560547
p mean is: tensor(-0.0565, device='cuda:2')
epoch:  14000 quantization_loss:  0.06801852583885193
p mean is: tensor(-0.0616, device='cuda:2')
epoch:  15000 quantization_loss:  0.06839025020599365
p mean is: tensor(-0.0670, device='cuda:2')
epoch:  16000 quantization_loss:  0.06790519505739212
p mean is: tensor(-0.0724, device='cuda:2')
epoch:  17000 quantization_loss:  0.06778580695390701
p mean is: tensor(-0.0780, device='cuda:2')
epoch:  18000 quantization_loss:  0.06751783937215805
p mean is: tensor(-0.0837, device='cuda:2')
epoch:  19000 quantization_loss:  0.06450679153203964
p mean is: tensor(-0.0898, device='cuda:2')
epoch:  20000 quantization_loss:  0.059321898967027664
p mean is: tensor(-0.0971, device='cuda:2')
epoch:  21000 quantization_loss:  0.05360616371035576
p mean is: tensor(-0.1059, device='cuda:2')
epoch:  22000 quantization_loss:  0.04972199723124504
p mean is: tensor(-0.1168, device='cuda:2')
epoch:  23000 quantization_loss:  0.04679179936647415
p mean is: tensor(-0.1305, device='cuda:2')
epoch:  24000 quantization_loss:  0.040605638176202774
p mean is: tensor(-0.1478, device='cuda:2')
epoch:  25000 quantization_loss:  0.03653240576386452
p mean is: tensor(-0.1694, device='cuda:2')
epoch:  26000 quantization_loss:  0.03411915525794029
p mean is: tensor(-0.1954, device='cuda:2')
epoch:  27000 quantization_loss:  0.031984832137823105
p mean is: tensor(-0.2268, device='cuda:2')
epoch:  28000 quantization_loss:  0.0313287228345871
p mean is: tensor(-0.2635, device='cuda:2')
epoch:  29000 quantization_loss:  0.029553083702921867
p mean is: tensor(-0.3049, device='cuda:2')
epoch:  30000 quantization_loss:  0.028569035232067108
p mean is: tensor(-0.3493, device='cuda:2')
epoch:  31000 quantization_loss:  0.0273613128811121
p mean is: tensor(-0.3956, device='cuda:2')
epoch:  32000 quantization_loss:  0.026681099086999893
p mean is: tensor(-0.4420, device='cuda:2')
epoch:  33000 quantization_loss:  0.026429621502757072
p mean is: tensor(-0.4876, device='cuda:2')
epoch:  34000 quantization_loss:  0.02600068598985672
p mean is: tensor(-0.5314, device='cuda:2')
epoch:  35000 quantization_loss:  0.023684831336140633
p mean is: tensor(-0.5726, device='cuda:2')
epoch:  36000 quantization_loss:  0.023260170593857765
p mean is: tensor(-0.6101, device='cuda:2')
epoch:  37000 quantization_loss:  0.02316277101635933
p mean is: tensor(-0.6440, device='cuda:2')
epoch:  38000 quantization_loss:  0.02260054647922516
p mean is: tensor(-0.6744, device='cuda:2')
epoch:  39000 quantization_loss:  0.022449973970651627
p mean is: tensor(-0.7015, device='cuda:2')
epoch:  40000 quantization_loss:  0.02220221608877182
p mean is: tensor(-0.7255, device='cuda:2')
epoch:  41000 quantization_loss:  0.021951748058199883
p mean is: tensor(-0.7466, device='cuda:2')
epoch:  42000 quantization_loss:  0.02184438519179821
p mean is: tensor(-0.7650, device='cuda:2')
epoch:  43000 quantization_loss:  0.02168889157474041
p mean is: tensor(-0.7811, device='cuda:2')
epoch:  44000 quantization_loss:  0.021473370492458344
p mean is: tensor(-0.7952, device='cuda:2')
epoch:  45000 quantization_loss:  0.02141040563583374
p mean is: tensor(-0.8076, device='cuda:2')
epoch:  46000 quantization_loss:  0.02125614881515503
p mean is: tensor(-0.8184, device='cuda:2')
epoch:  47000 quantization_loss:  0.02118092216551304
p mean is: tensor(-0.8279, device='cuda:2')
epoch:  48000 quantization_loss:  0.021094532683491707
p mean is: tensor(-0.8360, device='cuda:2')
epoch:  49000 quantization_loss:  0.021041082218289375
p mean is: tensor(-0.8433, device='cuda:2')
epoch:  50000 quantization_loss:  0.02095947228372097
p mean is: tensor(-0.8498, device='cuda:2')
epoch:  51000 quantization_loss:  0.020897328853607178
p mean is: tensor(-0.8555, device='cuda:2')
epoch:  52000 quantization_loss:  0.0208879467099905
p mean is: tensor(-0.8606, device='cuda:2')
epoch:  53000 quantization_loss:  0.020823506638407707
p mean is: tensor(-0.8653, device='cuda:2')
epoch:  54000 quantization_loss:  0.02076892927289009
p mean is: tensor(-0.8692, device='cuda:2')
epoch:  55000 quantization_loss:  0.02074083313345909
p mean is: tensor(-0.8729, device='cuda:2')
epoch:  56000 quantization_loss:  0.02071312442421913
p mean is: tensor(-0.8762, device='cuda:2')
epoch:  57000 quantization_loss:  0.020681126043200493
p mean is: tensor(-0.8794, device='cuda:2')
epoch:  58000 quantization_loss:  0.020670440047979355
p mean is: tensor(-0.8823, device='cuda:2')
epoch:  59000 quantization_loss:  0.020610613748431206
p mean is: tensor(-0.8849, device='cuda:2')
epoch:  60000 quantization_loss:  0.02058906853199005
p mean is: tensor(-0.8873, device='cuda:2')
epoch:  61000 quantization_loss:  0.020581098273396492
p mean is: tensor(-0.8896, device='cuda:2')
epoch:  62000 quantization_loss:  0.02058510109782219
p mean is: tensor(-0.8917, device='cuda:2')
epoch:  63000 quantization_loss:  0.020558753982186317
p mean is: tensor(-0.8936, device='cuda:2')
epoch:  64000 quantization_loss:  0.020549103617668152
p mean is: tensor(-0.8954, device='cuda:2')
epoch:  65000 quantization_loss:  0.02051553688943386
p mean is: tensor(-0.8971, device='cuda:2')
epoch:  66000 quantization_loss:  0.020523525774478912
p mean is: tensor(-0.8988, device='cuda:2')
epoch:  67000 quantization_loss:  0.02050692029297352
p mean is: tensor(-0.9003, device='cuda:2')
epoch:  68000 quantization_loss:  0.020490750670433044
p mean is: tensor(-0.9017, device='cuda:2')
epoch:  69000 quantization_loss:  0.02048114687204361
p mean is: tensor(-0.9032, device='cuda:2')
epoch:  70000 quantization_loss:  0.02048797532916069
p mean is: tensor(-0.9046, device='cuda:2')
epoch:  71000 quantization_loss:  0.02045588567852974
p mean is: tensor(-0.9058, device='cuda:2')
epoch:  72000 quantization_loss:  0.020463496446609497
p mean is: tensor(-0.9071, device='cuda:2')
epoch:  73000 quantization_loss:  0.02045120857656002
p mean is: tensor(-0.9083, device='cuda:2')
epoch:  74000 quantization_loss:  0.020450303331017494
p mean is: tensor(-0.9094, device='cuda:2')
epoch:  75000 quantization_loss:  0.020436489954590797
p mean is: tensor(-0.9104, device='cuda:2')
epoch:  76000 quantization_loss:  0.02042861469089985
p mean is: tensor(-0.9114, device='cuda:2')
epoch:  77000 quantization_loss:  0.02044089138507843
p mean is: tensor(-0.9124, device='cuda:2')
epoch:  78000 quantization_loss:  0.020415065810084343
p mean is: tensor(-0.9134, device='cuda:2')
epoch:  79000 quantization_loss:  0.020412884652614594
p mean is: tensor(-0.9143, device='cuda:2')
here
1.0.1.1.weight       | nonzeros =      38 /     128             ( 29.69%) | total_pruned =      90 | shape = torch.Size([4, 32, 1, 1])
1.0.1.1.bias         | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.0.2.weight         | nonzeros =       1 /       4             ( 25.00%) | total_pruned =       3 | shape = torch.Size([4])
1.0.2.bias           | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.1.1.weight       | nonzeros =    1760 /    4608             ( 38.19%) | total_pruned =    2848 | shape = torch.Size([16, 32, 3, 3])
1.1.1.1.bias         | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.2.weight         | nonzeros =      13 /      16             ( 81.25%) | total_pruned =       3 | shape = torch.Size([16])
1.1.2.bias           | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.4.1.weight       | nonzeros =     640 /    2304             ( 27.78%) | total_pruned =    1664 | shape = torch.Size([16, 16, 3, 3])
1.1.4.1.bias         | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.5.weight         | nonzeros =      11 /      16             ( 68.75%) | total_pruned =       5 | shape = torch.Size([16])
1.1.5.bias           | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.7.1.0.1.1.weight | nonzeros =       7 /      64             ( 10.94%) | total_pruned =      57 | shape = torch.Size([4, 16, 1, 1])
1.1.7.1.0.1.1.bias   | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.0.2.weight   | nonzeros =       1 /       4             ( 25.00%) | total_pruned =       3 | shape = torch.Size([4])
1.1.7.1.0.2.bias     | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.1.1.weight | nonzeros =    1117 /    4608             ( 24.24%) | total_pruned =    3491 | shape = torch.Size([32, 16, 3, 3])
1.1.7.1.1.1.1.bias   | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.2.weight   | nonzeros =      23 /      32             ( 71.88%) | total_pruned =       9 | shape = torch.Size([32])
1.1.7.1.1.2.bias     | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.4.1.weight | nonzeros =    2409 /    9216             ( 26.14%) | total_pruned =    6807 | shape = torch.Size([32, 32, 3, 3])
1.1.7.1.1.4.1.bias   | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.5.weight   | nonzeros =      25 /      32             ( 78.12%) | total_pruned =       7 | shape = torch.Size([32])
1.1.7.1.1.5.bias     | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.7.1.0.1.1.weight | nonzeros =      14 /     128             ( 10.94%) | total_pruned =     114 | shape = torch.Size([4, 32, 1, 1])
1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.0.2.weight | nonzeros =       1 /       4             ( 25.00%) | total_pruned =       3 | shape = torch.Size([4])
1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    4000 /   18432             ( 21.70%) | total_pruned =   14432 | shape = torch.Size([64, 32, 3, 3])
1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.2.weight | nonzeros =      45 /      64             ( 70.31%) | total_pruned =      19 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.4.1.weight | nonzeros =    4332 /   36864             ( 11.75%) | total_pruned =   32532 | shape = torch.Size([64, 64, 3, 3])
1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.5.weight | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.7.1.0.1.1.weight | nonzeros =      21 /     256             (  8.20%) | total_pruned =     235 | shape = torch.Size([4, 64, 1, 1])
1.1.7.1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.0.2.weight | nonzeros =       1 /       4             ( 25.00%) | total_pruned =       3 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    2808 /   73728             (  3.81%) | total_pruned =   70920 | shape = torch.Size([128, 64, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.2.weight | nonzeros =      74 /     128             ( 57.81%) | total_pruned =      54 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.4.1.weight | nonzeros =    3104 /  147456             (  2.11%) | total_pruned =  144352 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.5.weight | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.weight | nonzeros =      46 /     512             (  8.98%) | total_pruned =     466 | shape = torch.Size([4, 128, 1, 1])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.weight | nonzeros =       1 /       4             ( 25.00%) | total_pruned =       3 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.weight | nonzeros =     831 /  147456             (  0.56%) | total_pruned =  146625 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.weight | nonzeros =      73 /     128             ( 57.03%) | total_pruned =      55 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.weight | nonzeros =    2262 /  147456             (  1.53%) | total_pruned =  145194 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.weight | nonzeros =      85 /     128             ( 66.41%) | total_pruned =      43 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.weight | nonzeros =      35 /     512             (  6.84%) | total_pruned =     477 | shape = torch.Size([4, 128, 1, 1])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.weight | nonzeros =       1 /       4             ( 25.00%) | total_pruned =       3 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    5620 /  147456             (  3.81%) | total_pruned =  141836 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.weight | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.weight | nonzeros =   11791 /  147456             (  8.00%) | total_pruned =  135665 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.weight | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.2.weight | nonzeros =      66 /     132             ( 50.00%) | total_pruned =      66 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.2.bias | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.3.1.weight | nonzeros =   22320 /  152064             ( 14.68%) | total_pruned =  129744 | shape = torch.Size([128, 132, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.4.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.2.weight | nonzeros =      84 /     132             ( 63.64%) | total_pruned =      48 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.2.bias | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.3.1.weight | nonzeros =   24272 /  152064             ( 15.96%) | total_pruned =  127792 | shape = torch.Size([128, 132, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.4.weight | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.2.weight | nonzeros =      68 /     132             ( 51.52%) | total_pruned =      64 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.2.bias | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.3.1.weight | nonzeros =   22747 /  152064             ( 14.96%) | total_pruned =  129317 | shape = torch.Size([128, 132, 3, 3])
1.1.7.1.1.7.1.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.4.weight | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.2.weight | nonzeros =      79 /     132             ( 59.85%) | total_pruned =      53 | shape = torch.Size([132])
1.1.7.1.1.7.2.bias   | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.3.1.weight | nonzeros =   11324 /   76032             ( 14.89%) | total_pruned =   64708 | shape = torch.Size([64, 132, 3, 3])
1.1.7.1.1.7.3.1.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.4.weight | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
1.1.7.1.1.7.4.bias   | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.2.weight       | nonzeros =      34 /      68             ( 50.00%) | total_pruned =      34 | shape = torch.Size([68])
1.1.7.2.bias         | nonzeros =       0 /      68             (  0.00%) | total_pruned =      68 | shape = torch.Size([68])
1.1.7.3.1.weight     | nonzeros =    1807 /   19584             (  9.23%) | total_pruned =   17777 | shape = torch.Size([32, 68, 3, 3])
1.1.7.3.1.bias       | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.4.weight       | nonzeros =      12 /      32             ( 37.50%) | total_pruned =      20 | shape = torch.Size([32])
1.1.7.4.bias         | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
2.weight             | nonzeros =      12 /      36             ( 33.33%) | total_pruned =      24 | shape = torch.Size([36])
2.bias               | nonzeros =       0 /      36             (  0.00%) | total_pruned =      36 | shape = torch.Size([36])
3.1.weight           | nonzeros =     806 /    5184             ( 15.55%) | total_pruned =    4378 | shape = torch.Size([16, 36, 3, 3])
3.1.bias             | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
4.weight             | nonzeros =      16 /      16             (100.00%) | total_pruned =       0 | shape = torch.Size([16])
4.bias               | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
6.1.weight           | nonzeros =      31 /      48             ( 64.58%) | total_pruned =      17 | shape = torch.Size([3, 16, 1, 1])
6.1.bias             | nonzeros =       1 /       3             ( 33.33%) | total_pruned =       2 | shape = torch.Size([3])
alive: 125401, pruned : 1326082, total: 1451483, Compression rate :      11.57x  ( 91.36% pruned)
PSNR of output image is:  19.013718373431537
Experiment done
