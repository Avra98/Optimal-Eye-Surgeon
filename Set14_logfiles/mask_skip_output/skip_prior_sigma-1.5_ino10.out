(3, 512, 512)
Starting vanilla DIP on 10 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '20.16891882578787'
(3, 512, 512) (3, 512, 512) torch.Size([1, 32, 512, 512])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/mask/10/skip/det/-1.5/1e-09
(3, 512, 512)
Starting vanilla DIP on 10 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '20.165238820344136'
(3, 512, 512) (3, 512, 512) torch.Size([1, 32, 512, 512])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/mask/10/skip/det/-1.5/1e-09
epoch:  0 quantization_loss:  0.05990515276789665
p mean is: tensor(-0.0003, device='cuda:1')
epoch:  1000 quantization_loss:  0.05755891278386116
p mean is: tensor(-0.0111, device='cuda:1')
epoch:  2000 quantization_loss:  0.056555576622486115
p mean is: tensor(-0.0188, device='cuda:1')
epoch:  3000 quantization_loss:  0.05575020611286163
p mean is: tensor(-0.0263, device='cuda:1')
epoch:  4000 quantization_loss:  0.05536910146474838
p mean is: tensor(-0.0341, device='cuda:1')
epoch:  5000 quantization_loss:  0.05566968023777008
p mean is: tensor(-0.0419, device='cuda:1')
epoch:  6000 quantization_loss:  0.054827455431222916
p mean is: tensor(-0.0493, device='cuda:1')
epoch:  7000 quantization_loss:  0.056692492216825485
p mean is: tensor(-0.0573, device='cuda:1')
epoch:  8000 quantization_loss:  0.050169937312603
p mean is: tensor(-0.0658, device='cuda:1')
epoch:  9000 quantization_loss:  0.04178060591220856
p mean is: tensor(-0.0752, device='cuda:1')
epoch:  10000 quantization_loss:  0.036112748086452484
p mean is: tensor(-0.0866, device='cuda:1')
epoch:  11000 quantization_loss:  0.03317435458302498
p mean is: tensor(-0.1009, device='cuda:1')
epoch:  12000 quantization_loss:  0.032382555305957794
p mean is: tensor(-0.1196, device='cuda:1')
epoch:  13000 quantization_loss:  0.03045164979994297
p mean is: tensor(-0.1436, device='cuda:1')
epoch:  14000 quantization_loss:  0.028782812878489494
p mean is: tensor(-0.1745, device='cuda:1')
epoch:  15000 quantization_loss:  0.02782241441309452
p mean is: tensor(-0.2120, device='cuda:1')
epoch:  16000 quantization_loss:  0.027162648737430573
p mean is: tensor(-0.2570, device='cuda:1')
epoch:  17000 quantization_loss:  0.027302149683237076
p mean is: tensor(-0.3095, device='cuda:1')
epoch:  18000 quantization_loss:  0.026113614439964294
p mean is: tensor(-0.3690, device='cuda:1')
epoch:  19000 quantization_loss:  0.02591051533818245
p mean is: tensor(-0.4336, device='cuda:1')
epoch:  20000 quantization_loss:  0.025479156523942947
p mean is: tensor(-0.5014, device='cuda:1')
epoch:  21000 quantization_loss:  0.024967066943645477
p mean is: tensor(-0.5697, device='cuda:1')
epoch:  22000 quantization_loss:  0.024686060845851898
p mean is: tensor(-0.6367, device='cuda:1')
epoch:  23000 quantization_loss:  0.024527423083782196
p mean is: tensor(-0.7009, device='cuda:1')
epoch:  24000 quantization_loss:  0.024263184517621994
p mean is: tensor(-0.7610, device='cuda:1')
epoch:  25000 quantization_loss:  0.024214573204517365
p mean is: tensor(-0.8164, device='cuda:1')
epoch:  26000 quantization_loss:  0.024044809862971306
p mean is: tensor(-0.8673, device='cuda:1')
epoch:  27000 quantization_loss:  0.024154607206583023
p mean is: tensor(-0.9133, device='cuda:1')
epoch:  28000 quantization_loss:  0.023759907111525536
p mean is: tensor(-0.9549, device='cuda:1')
epoch:  29000 quantization_loss:  0.02470320649445057
p mean is: tensor(-0.9926, device='cuda:1')
epoch:  30000 quantization_loss:  0.023606780916452408
p mean is: tensor(-1.0264, device='cuda:1')
epoch:  31000 quantization_loss:  0.023371445015072823
p mean is: tensor(-1.0572, device='cuda:1')
epoch:  32000 quantization_loss:  0.023375622928142548
p mean is: tensor(-1.0850, device='cuda:1')
epoch:  33000 quantization_loss:  0.023283615708351135
p mean is: tensor(-1.1102, device='cuda:1')
epoch:  34000 quantization_loss:  0.023254351690411568
p mean is: tensor(-1.1332, device='cuda:1')
epoch:  35000 quantization_loss:  0.023229943588376045
p mean is: tensor(-1.1540, device='cuda:1')
epoch:  36000 quantization_loss:  0.023109428584575653
p mean is: tensor(-1.1731, device='cuda:1')
epoch:  37000 quantization_loss:  0.022933967411518097
p mean is: tensor(-1.1904, device='cuda:1')
epoch:  38000 quantization_loss:  0.022922871634364128
p mean is: tensor(-1.2064, device='cuda:1')
epoch:  39000 quantization_loss:  0.023006629198789597
p mean is: tensor(-1.2212, device='cuda:1')
epoch:  40000 quantization_loss:  0.02285664901137352
p mean is: tensor(-1.2349, device='cuda:1')
epoch:  41000 quantization_loss:  0.02292069047689438
p mean is: tensor(-1.2475, device='cuda:1')
epoch:  42000 quantization_loss:  0.02287169173359871
p mean is: tensor(-1.2594, device='cuda:1')
epoch:  43000 quantization_loss:  0.022846581414341927
p mean is: tensor(-1.2702, device='cuda:1')
epoch:  44000 quantization_loss:  0.022802677005529404
p mean is: tensor(-1.2805, device='cuda:1')
epoch:  45000 quantization_loss:  0.022786539047956467
p mean is: tensor(-1.2900, device='cuda:1')
epoch:  46000 quantization_loss:  0.022760670632123947
p mean is: tensor(-1.2990, device='cuda:1')
epoch:  47000 quantization_loss:  0.022745536640286446
p mean is: tensor(-1.3075, device='cuda:1')
epoch:  48000 quantization_loss:  0.022648870944976807
p mean is: tensor(-1.3155, device='cuda:1')
epoch:  49000 quantization_loss:  0.022635189816355705
p mean is: tensor(-1.3230, device='cuda:1')
epoch:  50000 quantization_loss:  0.022685933858156204
p mean is: tensor(-1.3303, device='cuda:1')
epoch:  51000 quantization_loss:  0.022526955232024193
p mean is: tensor(-1.3373, device='cuda:1')
epoch:  52000 quantization_loss:  0.0226516742259264
p mean is: tensor(-1.3439, device='cuda:1')
epoch:  53000 quantization_loss:  0.022640326991677284
p mean is: tensor(-1.3501, device='cuda:1')
epoch:  54000 quantization_loss:  0.02263956144452095
p mean is: tensor(-1.3561, device='cuda:1')
epoch:  55000 quantization_loss:  0.022690672427415848
p mean is: tensor(-1.3618, device='cuda:1')
epoch:  56000 quantization_loss:  0.022619646042585373
p mean is: tensor(-1.3672, device='cuda:1')
epoch:  57000 quantization_loss:  0.022598598152399063
p mean is: tensor(-1.3725, device='cuda:1')
epoch:  58000 quantization_loss:  0.02259475365281105
p mean is: tensor(-1.3775, device='cuda:1')
epoch:  59000 quantization_loss:  0.02267340011894703
p mean is: tensor(-1.3824, device='cuda:1')
epoch:  60000 quantization_loss:  0.022569691762328148
p mean is: tensor(-1.3870, device='cuda:1')
epoch:  61000 quantization_loss:  0.022462503984570503
p mean is: tensor(-1.3915, device='cuda:1')
epoch:  62000 quantization_loss:  0.022559843957424164
p mean is: tensor(-1.3959, device='cuda:1')
epoch:  63000 quantization_loss:  0.02249266393482685
p mean is: tensor(-1.4001, device='cuda:1')
epoch:  64000 quantization_loss:  0.022559309378266335
p mean is: tensor(-1.4042, device='cuda:1')
epoch:  65000 quantization_loss:  0.022544020786881447
p mean is: tensor(-1.4083, device='cuda:1')
epoch:  66000 quantization_loss:  0.022646039724349976
p mean is: tensor(-1.4120, device='cuda:1')
epoch:  67000 quantization_loss:  0.02252945676445961
p mean is: tensor(-1.4156, device='cuda:1')
epoch:  68000 quantization_loss:  0.022530782967805862
p mean is: tensor(-1.4191, device='cuda:1')
epoch:  69000 quantization_loss:  0.02262577787041664
p mean is: tensor(-1.4227, device='cuda:1')
epoch:  70000 quantization_loss:  0.022518441081047058
p mean is: tensor(-1.4261, device='cuda:1')
epoch:  71000 quantization_loss:  0.022521909326314926
p mean is: tensor(-1.4294, device='cuda:1')
epoch:  72000 quantization_loss:  0.022523919120430946
p mean is: tensor(-1.4326, device='cuda:1')
epoch:  73000 quantization_loss:  0.022515906020998955
p mean is: tensor(-1.4357, device='cuda:1')
epoch:  74000 quantization_loss:  0.022519903257489204
p mean is: tensor(-1.4388, device='cuda:1')
epoch:  75000 quantization_loss:  0.022394869476556778
p mean is: tensor(-1.4420, device='cuda:1')
epoch:  76000 quantization_loss:  0.022504542022943497
p mean is: tensor(-1.4449, device='cuda:1')
epoch:  77000 quantization_loss:  0.022504033520817757
p mean is: tensor(-1.4479, device='cuda:1')
epoch:  78000 quantization_loss:  0.022499671205878258
p mean is: tensor(-1.4507, device='cuda:1')
epoch:  79000 quantization_loss:  0.022495217621326447
p mean is: tensor(-1.4535, device='cuda:1')
here
1.0.1.1.weight       | nonzeros =       1 /     128             (  0.78%) | total_pruned =     127 | shape = torch.Size([4, 32, 1, 1])
1.0.1.1.bias         | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.0.2.weight         | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.0.2.bias           | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.1.1.weight       | nonzeros =    1158 /    4608             ( 25.13%) | total_pruned =    3450 | shape = torch.Size([16, 32, 3, 3])
1.1.1.1.bias         | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.2.weight         | nonzeros =       9 /      16             ( 56.25%) | total_pruned =       7 | shape = torch.Size([16])
1.1.2.bias           | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.4.1.weight       | nonzeros =     536 /    2304             ( 23.26%) | total_pruned =    1768 | shape = torch.Size([16, 16, 3, 3])
1.1.4.1.bias         | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.5.weight         | nonzeros =      14 /      16             ( 87.50%) | total_pruned =       2 | shape = torch.Size([16])
1.1.5.bias           | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.1.7.1.0.1.1.weight | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([4, 16, 1, 1])
1.1.7.1.0.1.1.bias   | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.0.2.weight   | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.0.2.bias     | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.1.1.weight | nonzeros =    1073 /    4608             ( 23.29%) | total_pruned =    3535 | shape = torch.Size([32, 16, 3, 3])
1.1.7.1.1.1.1.bias   | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.2.weight   | nonzeros =      23 /      32             ( 71.88%) | total_pruned =       9 | shape = torch.Size([32])
1.1.7.1.1.2.bias     | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.4.1.weight | nonzeros =    1940 /    9216             ( 21.05%) | total_pruned =    7276 | shape = torch.Size([32, 32, 3, 3])
1.1.7.1.1.4.1.bias   | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.5.weight   | nonzeros =      26 /      32             ( 81.25%) | total_pruned =       6 | shape = torch.Size([32])
1.1.7.1.1.5.bias     | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.1.1.7.1.0.1.1.weight | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([4, 32, 1, 1])
1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.0.2.weight | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    3148 /   18432             ( 17.08%) | total_pruned =   15284 | shape = torch.Size([64, 32, 3, 3])
1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.2.weight | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.4.1.weight | nonzeros =    3847 /   36864             ( 10.44%) | total_pruned =   33017 | shape = torch.Size([64, 64, 3, 3])
1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.5.weight | nonzeros =      47 /      64             ( 73.44%) | total_pruned =      17 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.1.1.7.1.0.1.1.weight | nonzeros =      20 /     256             (  7.81%) | total_pruned =     236 | shape = torch.Size([4, 64, 1, 1])
1.1.7.1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.0.2.weight | nonzeros =       1 /       4             ( 25.00%) | total_pruned =       3 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    5011 /   73728             (  6.80%) | total_pruned =   68717 | shape = torch.Size([128, 64, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.2.weight | nonzeros =      97 /     128             ( 75.78%) | total_pruned =      31 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.4.1.weight | nonzeros =    7962 /  147456             (  5.40%) | total_pruned =  139494 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.5.weight | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.weight | nonzeros =     199 /     512             ( 38.87%) | total_pruned =     313 | shape = torch.Size([4, 128, 1, 1])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.weight | nonzeros =       4 /       4             (100.00%) | total_pruned =       0 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    1503 /  147456             (  1.02%) | total_pruned =  145953 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.weight | nonzeros =      95 /     128             ( 74.22%) | total_pruned =      33 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.weight | nonzeros =    4370 /  147456             (  2.96%) | total_pruned =  143086 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.weight | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.weight | nonzeros =     150 /     512             ( 29.30%) | total_pruned =     362 | shape = torch.Size([4, 128, 1, 1])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.1.1.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.weight | nonzeros =       3 /       4             ( 75.00%) | total_pruned =       1 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.0.2.bias | nonzeros =       0 /       4             (  0.00%) | total_pruned =       4 | shape = torch.Size([4])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.weight | nonzeros =    7416 /  147456             (  5.03%) | total_pruned =  140040 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.weight | nonzeros =      95 /     128             ( 74.22%) | total_pruned =      33 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.weight | nonzeros =   17224 /  147456             ( 11.68%) | total_pruned =  130232 | shape = torch.Size([128, 128, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.1.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.2.weight | nonzeros =      86 /     132             ( 65.15%) | total_pruned =      46 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.2.bias | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.3.1.weight | nonzeros =   26761 /  152064             ( 17.60%) | total_pruned =  125303 | shape = torch.Size([128, 132, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.4.weight | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.1.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.2.weight | nonzeros =      91 /     132             ( 68.94%) | total_pruned =      41 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.2.bias | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.1.1.7.3.1.weight | nonzeros =   28308 /  152064             ( 18.62%) | total_pruned =  123756 | shape = torch.Size([128, 132, 3, 3])
1.1.7.1.1.7.1.1.7.1.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.4.weight | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.1.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.2.weight | nonzeros =      82 /     132             ( 62.12%) | total_pruned =      50 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.2.bias | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.1.1.7.3.1.weight | nonzeros =   24526 /  152064             ( 16.13%) | total_pruned =  127538 | shape = torch.Size([128, 132, 3, 3])
1.1.7.1.1.7.1.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.4.weight | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
1.1.7.1.1.7.1.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.1.7.1.1.7.2.weight | nonzeros =      78 /     132             ( 59.09%) | total_pruned =      54 | shape = torch.Size([132])
1.1.7.1.1.7.2.bias   | nonzeros =       0 /     132             (  0.00%) | total_pruned =     132 | shape = torch.Size([132])
1.1.7.1.1.7.3.1.weight | nonzeros =   11217 /   76032             ( 14.75%) | total_pruned =   64815 | shape = torch.Size([64, 132, 3, 3])
1.1.7.1.1.7.3.1.bias | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.1.1.7.4.weight | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
1.1.7.1.1.7.4.bias   | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.1.7.2.weight       | nonzeros =      37 /      68             ( 54.41%) | total_pruned =      31 | shape = torch.Size([68])
1.1.7.2.bias         | nonzeros =       0 /      68             (  0.00%) | total_pruned =      68 | shape = torch.Size([68])
1.1.7.3.1.weight     | nonzeros =    1705 /   19584             (  8.71%) | total_pruned =   17879 | shape = torch.Size([32, 68, 3, 3])
1.1.7.3.1.bias       | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.1.7.4.weight       | nonzeros =      11 /      32             ( 34.38%) | total_pruned =      21 | shape = torch.Size([32])
1.1.7.4.bias         | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
2.weight             | nonzeros =      11 /      36             ( 30.56%) | total_pruned =      25 | shape = torch.Size([36])
2.bias               | nonzeros =       0 /      36             (  0.00%) | total_pruned =      36 | shape = torch.Size([36])
3.1.weight           | nonzeros =     674 /    5184             ( 13.00%) | total_pruned =    4510 | shape = torch.Size([16, 36, 3, 3])
3.1.bias             | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
4.weight             | nonzeros =      16 /      16             (100.00%) | total_pruned =       0 | shape = torch.Size([16])
4.bias               | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
6.1.weight           | nonzeros =      28 /      48             ( 58.33%) | total_pruned =      20 | shape = torch.Size([3, 16, 1, 1])
6.1.bias             | nonzeros =       2 /       3             ( 66.67%) | total_pruned =       1 | shape = torch.Size([3])
alive: 150201, pruned : 1301282, total: 1451483, Compression rate :       9.66x  ( 89.65% pruned)
PSNR of output image is:  18.696832082383242
Experiment done
