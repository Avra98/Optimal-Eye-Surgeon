Starting vanilla DIP on 8 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '21.515649647615952'
(3, 512, 512) (3, 512, 512) torch.Size([1, 32, 512, 512])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/mask/8/SAM(sigma=0.1,lr=0.01,decay=0,beta=0)/det/-0.7/1e-09
epoch:  0 quantization_loss:  0.16058507561683655
p mean is: tensor(-0.0001, device='cuda:1')
epoch:  1000 quantization_loss:  0.12945708632469177
p mean is: tensor(-0.0035, device='cuda:1')
epoch:  2000 quantization_loss:  0.12078991532325745
p mean is: tensor(-0.0058, device='cuda:1')
epoch:  3000 quantization_loss:  0.11636906117200851
p mean is: tensor(-0.0080, device='cuda:1')
epoch:  4000 quantization_loss:  0.11132856458425522
p mean is: tensor(-0.0102, device='cuda:1')
epoch:  5000 quantization_loss:  0.10332483798265457
p mean is: tensor(-0.0127, device='cuda:1')
epoch:  6000 quantization_loss:  0.09987449645996094
p mean is: tensor(-0.0160, device='cuda:1')
epoch:  7000 quantization_loss:  0.09185776859521866
p mean is: tensor(-0.0208, device='cuda:1')
epoch:  8000 quantization_loss:  0.08679051697254181
p mean is: tensor(-0.0269, device='cuda:1')
epoch:  9000 quantization_loss:  0.08477076888084412
p mean is: tensor(-0.0350, device='cuda:1')
epoch:  10000 quantization_loss:  0.0832461416721344
p mean is: tensor(-0.0458, device='cuda:1')
epoch:  11000 quantization_loss:  0.07854269444942474
p mean is: tensor(-0.0603, device='cuda:1')
epoch:  12000 quantization_loss:  0.07699678838253021
p mean is: tensor(-0.0786, device='cuda:1')
epoch:  13000 quantization_loss:  0.07606086134910583
p mean is: tensor(-0.1016, device='cuda:1')
epoch:  14000 quantization_loss:  0.07464446127414703
p mean is: tensor(-0.1295, device='cuda:1')
epoch:  15000 quantization_loss:  0.07406312227249146
p mean is: tensor(-0.1624, device='cuda:1')
epoch:  16000 quantization_loss:  0.0731605514883995
p mean is: tensor(-0.1992, device='cuda:1')
epoch:  17000 quantization_loss:  0.07267652451992035
p mean is: tensor(-0.2385, device='cuda:1')
epoch:  18000 quantization_loss:  0.07216700911521912
p mean is: tensor(-0.2789, device='cuda:1')
epoch:  19000 quantization_loss:  0.07180040329694748
p mean is: tensor(-0.3186, device='cuda:1')
epoch:  20000 quantization_loss:  0.07147576659917831
p mean is: tensor(-0.3565, device='cuda:1')
epoch:  21000 quantization_loss:  0.07101758569478989
p mean is: tensor(-0.3918, device='cuda:1')
epoch:  22000 quantization_loss:  0.07093124836683273
p mean is: tensor(-0.4241, device='cuda:1')
epoch:  23000 quantization_loss:  0.0706261619925499
p mean is: tensor(-0.4533, device='cuda:1')
epoch:  24000 quantization_loss:  0.07048774510622025
p mean is: tensor(-0.4794, device='cuda:1')
epoch:  25000 quantization_loss:  0.07025554776191711
p mean is: tensor(-0.5024, device='cuda:1')
epoch:  26000 quantization_loss:  0.07011425495147705
p mean is: tensor(-0.5228, device='cuda:1')
epoch:  27000 quantization_loss:  0.07000648230314255
p mean is: tensor(-0.5408, device='cuda:1')
epoch:  28000 quantization_loss:  0.06986288726329803
p mean is: tensor(-0.5565, device='cuda:1')
epoch:  29000 quantization_loss:  0.06978128105401993
p mean is: tensor(-0.5705, device='cuda:1')
epoch:  30000 quantization_loss:  0.0696103423833847
p mean is: tensor(-0.5827, device='cuda:1')
epoch:  31000 quantization_loss:  0.06950780749320984
p mean is: tensor(-0.5933, device='cuda:1')
epoch:  32000 quantization_loss:  0.06941704452037811
p mean is: tensor(-0.6028, device='cuda:1')
epoch:  33000 quantization_loss:  0.06937108188867569
p mean is: tensor(-0.6112, device='cuda:1')
epoch:  34000 quantization_loss:  0.06928850710391998
p mean is: tensor(-0.6186, device='cuda:1')
epoch:  35000 quantization_loss:  0.06922305375337601
p mean is: tensor(-0.6251, device='cuda:1')
epoch:  36000 quantization_loss:  0.06911946833133698
p mean is: tensor(-0.6309, device='cuda:1')
epoch:  37000 quantization_loss:  0.06906792521476746
p mean is: tensor(-0.6361, device='cuda:1')
epoch:  38000 quantization_loss:  0.06901431828737259
p mean is: tensor(-0.6408, device='cuda:1')
epoch:  39000 quantization_loss:  0.06898966431617737
p mean is: tensor(-0.6450, device='cuda:1')
epoch:  40000 quantization_loss:  0.06896696984767914
p mean is: tensor(-0.6488, device='cuda:1')
epoch:  41000 quantization_loss:  0.06892681866884232
p mean is: tensor(-0.6522, device='cuda:1')
epoch:  42000 quantization_loss:  0.06887336075305939
p mean is: tensor(-0.6554, device='cuda:1')
epoch:  43000 quantization_loss:  0.06884872913360596
p mean is: tensor(-0.6582, device='cuda:1')
epoch:  44000 quantization_loss:  0.06882007420063019
p mean is: tensor(-0.6608, device='cuda:1')
epoch:  45000 quantization_loss:  0.06878679990768433
p mean is: tensor(-0.6633, device='cuda:1')
epoch:  46000 quantization_loss:  0.06878281384706497
p mean is: tensor(-0.6656, device='cuda:1')
epoch:  47000 quantization_loss:  0.06876257061958313
p mean is: tensor(-0.6677, device='cuda:1')
epoch:  48000 quantization_loss:  0.06873244047164917
p mean is: tensor(-0.6695, device='cuda:1')
epoch:  49000 quantization_loss:  0.06872471421957016
p mean is: tensor(-0.6712, device='cuda:1')
epoch:  50000 quantization_loss:  0.06869309395551682
p mean is: tensor(-0.6729, device='cuda:1')
epoch:  51000 quantization_loss:  0.06868242472410202
p mean is: tensor(-0.6745, device='cuda:1')
epoch:  52000 quantization_loss:  0.06872555613517761
p mean is: tensor(-0.6760, device='cuda:1')
epoch:  53000 quantization_loss:  0.06865236908197403
p mean is: tensor(-0.6774, device='cuda:1')
epoch:  54000 quantization_loss:  0.06864145398139954
p mean is: tensor(-0.6786, device='cuda:1')
epoch:  55000 quantization_loss:  0.06864131987094879
p mean is: tensor(-0.6799, device='cuda:1')
epoch:  56000 quantization_loss:  0.06862048804759979
p mean is: tensor(-0.6811, device='cuda:1')
epoch:  57000 quantization_loss:  0.068612240254879
p mean is: tensor(-0.6822, device='cuda:1')
epoch:  58000 quantization_loss:  0.06861092895269394
p mean is: tensor(-0.6834, device='cuda:1')
epoch:  59000 quantization_loss:  0.06859184056520462
p mean is: tensor(-0.6844, device='cuda:1')
epoch:  60000 quantization_loss:  0.06859560310840607
p mean is: tensor(-0.6855, device='cuda:1')
epoch:  61000 quantization_loss:  0.06857061386108398
p mean is: tensor(-0.6865, device='cuda:1')
epoch:  62000 quantization_loss:  0.06856201589107513
p mean is: tensor(-0.6874, device='cuda:1')
epoch:  63000 quantization_loss:  0.06855838745832443
p mean is: tensor(-0.6883, device='cuda:1')
epoch:  64000 quantization_loss:  0.0685395747423172
p mean is: tensor(-0.6892, device='cuda:1')
epoch:  65000 quantization_loss:  0.06857287138700485
p mean is: tensor(-0.6900, device='cuda:1')
epoch:  66000 quantization_loss:  0.06854283809661865
p mean is: tensor(-0.6908, device='cuda:1')
epoch:  67000 quantization_loss:  0.06854220479726791
p mean is: tensor(-0.6915, device='cuda:1')
epoch:  68000 quantization_loss:  0.06852980703115463
p mean is: tensor(-0.6922, device='cuda:1')
epoch:  69000 quantization_loss:  0.0685245618224144
p mean is: tensor(-0.6929, device='cuda:1')
epoch:  70000 quantization_loss:  0.06852302700281143
p mean is: tensor(-0.6936, device='cuda:1')
epoch:  71000 quantization_loss:  0.06850855052471161
p mean is: tensor(-0.6942, device='cuda:1')
epoch:  72000 quantization_loss:  0.06854629516601562
p mean is: tensor(-0.6949, device='cuda:1')
epoch:  73000 quantization_loss:  0.06851103156805038
p mean is: tensor(-0.6955, device='cuda:1')
epoch:  74000 quantization_loss:  0.06851092725992203
p mean is: tensor(-0.6962, device='cuda:1')
epoch:  75000 quantization_loss:  0.0684930831193924
p mean is: tensor(-0.6968, device='cuda:1')
epoch:  76000 quantization_loss:  0.06849446147680283
p mean is: tensor(-0.6974, device='cuda:1')
epoch:  77000 quantization_loss:  0.06848947703838348
p mean is: tensor(-0.6980, device='cuda:1')
epoch:  78000 quantization_loss:  0.06849133223295212
p mean is: tensor(-0.6985, device='cuda:1')
epoch:  79000 quantization_loss:  0.06848970800638199
p mean is: tensor(-0.6990, device='cuda:1')
here
1.1.1.weight         | nonzeros =     995 /   12800             (  7.77%) | total_pruned =   11805 | shape = torch.Size([16, 32, 5, 5])
1.1.1.bias           | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.2.weight           | nonzeros =       7 /      16             ( 43.75%) | total_pruned =       9 | shape = torch.Size([16])
1.2.bias             | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.4.1.weight         | nonzeros =     104 /    6400             (  1.62%) | total_pruned =    6296 | shape = torch.Size([16, 16, 5, 5])
1.4.1.bias           | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.5.weight           | nonzeros =       6 /      16             ( 37.50%) | total_pruned =      10 | shape = torch.Size([16])
1.5.bias             | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
1.7.1.1.1.weight     | nonzeros =     121 /   12800             (  0.95%) | total_pruned =   12679 | shape = torch.Size([32, 16, 5, 5])
1.7.1.1.1.bias       | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.7.1.2.weight       | nonzeros =      13 /      32             ( 40.62%) | total_pruned =      19 | shape = torch.Size([32])
1.7.1.2.bias         | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.7.1.4.1.weight     | nonzeros =     252 /   25600             (  0.98%) | total_pruned =   25348 | shape = torch.Size([32, 32, 5, 5])
1.7.1.4.1.bias       | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.7.1.5.weight       | nonzeros =       9 /      32             ( 28.12%) | total_pruned =      23 | shape = torch.Size([32])
1.7.1.5.bias         | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.7.1.7.1.1.1.weight | nonzeros =     300 /   51200             (  0.59%) | total_pruned =   50900 | shape = torch.Size([64, 32, 5, 5])
1.7.1.7.1.1.1.bias   | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.7.1.7.1.2.weight   | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
1.7.1.7.1.2.bias     | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.7.1.7.1.4.1.weight | nonzeros =     530 /  102400             (  0.52%) | total_pruned =  101870 | shape = torch.Size([64, 64, 5, 5])
1.7.1.7.1.4.1.bias   | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.7.1.7.1.5.weight   | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
1.7.1.7.1.5.bias     | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.7.1.7.1.7.1.1.1.weight | nonzeros =     459 /  204800             (  0.22%) | total_pruned =  204341 | shape = torch.Size([128, 64, 5, 5])
1.7.1.7.1.7.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.2.weight | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
1.7.1.7.1.7.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.4.1.weight | nonzeros =    1015 /  409600             (  0.25%) | total_pruned =  408585 | shape = torch.Size([128, 128, 5, 5])
1.7.1.7.1.7.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.5.weight | nonzeros =      58 /     128             ( 45.31%) | total_pruned =      70 | shape = torch.Size([128])
1.7.1.7.1.7.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.1.1.weight | nonzeros =    1682 /  409600             (  0.41%) | total_pruned =  407918 | shape = torch.Size([128, 128, 5, 5])
1.7.1.7.1.7.1.7.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.2.weight | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.4.1.weight | nonzeros =    6323 /  409600             (  1.54%) | total_pruned =  403277 | shape = torch.Size([128, 128, 5, 5])
1.7.1.7.1.7.1.7.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.5.weight | nonzeros =     101 /     128             ( 78.91%) | total_pruned =      27 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.1.1.1.weight | nonzeros =   19261 /  409600             (  4.70%) | total_pruned =  390339 | shape = torch.Size([128, 128, 5, 5])
1.7.1.7.1.7.1.7.1.7.1.1.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.1.2.weight | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.1.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.1.4.1.weight | nonzeros =   52852 /  409600             ( 12.90%) | total_pruned =  356748 | shape = torch.Size([128, 128, 5, 5])
1.7.1.7.1.7.1.7.1.7.1.4.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.1.5.weight | nonzeros =      97 /     128             ( 75.78%) | total_pruned =      31 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.1.5.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.2.weight | nonzeros =      97 /     128             ( 75.78%) | total_pruned =      31 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.3.1.weight | nonzeros =   35886 /  147456             ( 24.34%) | total_pruned =  111570 | shape = torch.Size([128, 128, 3, 3])
1.7.1.7.1.7.1.7.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.4.weight | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.2.weight | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.2.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.3.1.weight | nonzeros =   30727 /  147456             ( 20.84%) | total_pruned =  116729 | shape = torch.Size([128, 128, 3, 3])
1.7.1.7.1.7.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.4.weight | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
1.7.1.7.1.7.1.7.4.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.2.weight | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
1.7.1.7.1.7.2.bias   | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.3.1.weight | nonzeros =   22549 /  147456             ( 15.29%) | total_pruned =  124907 | shape = torch.Size([128, 128, 3, 3])
1.7.1.7.1.7.3.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.1.7.4.weight | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
1.7.1.7.1.7.4.bias   | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.2.weight     | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
1.7.1.7.2.bias       | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
1.7.1.7.3.1.weight   | nonzeros =    7110 /   73728             (  9.64%) | total_pruned =   66618 | shape = torch.Size([64, 128, 3, 3])
1.7.1.7.3.1.bias     | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.7.1.7.4.weight     | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
1.7.1.7.4.bias       | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.7.2.weight         | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
1.7.2.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
1.7.3.1.weight       | nonzeros =    1535 /   18432             (  8.33%) | total_pruned =   16897 | shape = torch.Size([32, 64, 3, 3])
1.7.3.1.bias         | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
1.7.4.weight         | nonzeros =      14 /      32             ( 43.75%) | total_pruned =      18 | shape = torch.Size([32])
1.7.4.bias           | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
2.weight             | nonzeros =      14 /      32             ( 43.75%) | total_pruned =      18 | shape = torch.Size([32])
2.bias               | nonzeros =       0 /      32             (  0.00%) | total_pruned =      32 | shape = torch.Size([32])
3.1.weight           | nonzeros =     930 /    4608             ( 20.18%) | total_pruned =    3678 | shape = torch.Size([16, 32, 3, 3])
3.1.bias             | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
4.weight             | nonzeros =      16 /      16             (100.00%) | total_pruned =       0 | shape = torch.Size([16])
4.bias               | nonzeros =       0 /      16             (  0.00%) | total_pruned =      16 | shape = torch.Size([16])
6.1.weight           | nonzeros =      26 /      48             ( 54.17%) | total_pruned =      22 | shape = torch.Size([3, 16, 1, 1])
6.1.bias             | nonzeros =       1 /       3             ( 33.33%) | total_pruned =       2 | shape = torch.Size([3])
alive: 183911, pruned : 2824956, total: 3008867, Compression rate :      16.36x  ( 93.89% pruned)
PSNR of output image is:  11.308378487836254
Experiment done
