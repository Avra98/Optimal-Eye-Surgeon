(3, 512, 512)
Starting vanilla DIP on 10 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '20.177946459488364'
[128, 128, 128, 128, 128]
Number of params in decoder: 100224
torch.Size([1, 128, 16, 16])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/decoder_mask/10/SAM(sigma=0.1,lr=0.01,decay=0,beta=0)/det/-2.5/1e-09
epoch:  0 quantization_loss:  0.060183584690093994
p mean is: tensor(-0.0004, device='cuda:1')
epoch:  1000 quantization_loss:  0.057726841419935226
p mean is: tensor(-0.0149, device='cuda:1')
epoch:  2000 quantization_loss:  0.0356530137360096
p mean is: tensor(-0.0223, device='cuda:1')
epoch:  3000 quantization_loss:  0.031112726777791977
p mean is: tensor(-0.0306, device='cuda:1')
epoch:  4000 quantization_loss:  0.02946067601442337
p mean is: tensor(-0.0406, device='cuda:1')
epoch:  5000 quantization_loss:  0.027728291228413582
p mean is: tensor(-0.0517, device='cuda:1')
epoch:  6000 quantization_loss:  0.026622137054800987
p mean is: tensor(-0.0654, device='cuda:1')
epoch:  7000 quantization_loss:  0.02554730325937271
p mean is: tensor(-0.0809, device='cuda:1')
epoch:  8000 quantization_loss:  0.02521125040948391
p mean is: tensor(-0.1004, device='cuda:1')
epoch:  9000 quantization_loss:  0.0242239311337471
p mean is: tensor(-0.1233, device='cuda:1')
epoch:  10000 quantization_loss:  0.023960528895258904
p mean is: tensor(-0.1496, device='cuda:1')
epoch:  11000 quantization_loss:  0.02366974763572216
p mean is: tensor(-0.1806, device='cuda:1')
epoch:  12000 quantization_loss:  0.023268021643161774
p mean is: tensor(-0.2160, device='cuda:1')
epoch:  13000 quantization_loss:  0.023061860352754593
p mean is: tensor(-0.2552, device='cuda:1')
epoch:  14000 quantization_loss:  0.02260560169816017
p mean is: tensor(-0.2976, device='cuda:1')
epoch:  15000 quantization_loss:  0.02240726538002491
p mean is: tensor(-0.3425, device='cuda:1')
epoch:  16000 quantization_loss:  0.02219969592988491
p mean is: tensor(-0.3878, device='cuda:1')
epoch:  17000 quantization_loss:  0.021993622183799744
p mean is: tensor(-0.4339, device='cuda:1')
epoch:  18000 quantization_loss:  0.021739061921834946
p mean is: tensor(-0.4803, device='cuda:1')
epoch:  19000 quantization_loss:  0.021513134241104126
p mean is: tensor(-0.5254, device='cuda:1')
epoch:  20000 quantization_loss:  0.02130105160176754
p mean is: tensor(-0.5678, device='cuda:1')
epoch:  21000 quantization_loss:  0.021511977538466454
p mean is: tensor(-0.6084, device='cuda:1')
epoch:  22000 quantization_loss:  0.0210577379912138
p mean is: tensor(-0.6479, device='cuda:1')
epoch:  23000 quantization_loss:  0.020763078704476357
p mean is: tensor(-0.6861, device='cuda:1')
epoch:  24000 quantization_loss:  0.02069024369120598
p mean is: tensor(-0.7214, device='cuda:1')
epoch:  25000 quantization_loss:  0.020448988303542137
p mean is: tensor(-0.7563, device='cuda:1')
epoch:  26000 quantization_loss:  0.02042822539806366
p mean is: tensor(-0.7898, device='cuda:1')
epoch:  27000 quantization_loss:  0.02037889137864113
p mean is: tensor(-0.8213, device='cuda:1')
epoch:  28000 quantization_loss:  0.020255737006664276
p mean is: tensor(-0.8501, device='cuda:1')
epoch:  29000 quantization_loss:  0.02017270214855671
p mean is: tensor(-0.8780, device='cuda:1')
epoch:  30000 quantization_loss:  0.019997814670205116
p mean is: tensor(-0.9046, device='cuda:1')
epoch:  31000 quantization_loss:  0.019845198839902878
p mean is: tensor(-0.9291, device='cuda:1')
epoch:  32000 quantization_loss:  0.019822491332888603
p mean is: tensor(-0.9517, device='cuda:1')
epoch:  33000 quantization_loss:  0.019778043031692505
p mean is: tensor(-0.9737, device='cuda:1')
epoch:  34000 quantization_loss:  0.01967211626470089
p mean is: tensor(-0.9944, device='cuda:1')
epoch:  35000 quantization_loss:  0.01956118270754814
p mean is: tensor(-1.0136, device='cuda:1')
epoch:  36000 quantization_loss:  0.019516168162226677
p mean is: tensor(-1.0315, device='cuda:1')
epoch:  37000 quantization_loss:  0.01947629638016224
p mean is: tensor(-1.0491, device='cuda:1')
epoch:  38000 quantization_loss:  0.019493628293275833
p mean is: tensor(-1.0642, device='cuda:1')
epoch:  39000 quantization_loss:  0.019367583096027374
p mean is: tensor(-1.0780, device='cuda:1')
epoch:  40000 quantization_loss:  0.019297795370221138
p mean is: tensor(-1.0919, device='cuda:1')
epoch:  41000 quantization_loss:  0.01925901509821415
p mean is: tensor(-1.1053, device='cuda:1')
epoch:  42000 quantization_loss:  0.01926853321492672
p mean is: tensor(-1.1165, device='cuda:1')
epoch:  43000 quantization_loss:  0.019223706796765327
p mean is: tensor(-1.1271, device='cuda:1')
epoch:  44000 quantization_loss:  0.019258441403508186
p mean is: tensor(-1.1372, device='cuda:1')
epoch:  45000 quantization_loss:  0.01916029304265976
p mean is: tensor(-1.1454, device='cuda:1')
epoch:  46000 quantization_loss:  0.019124703481793404
p mean is: tensor(-1.1536, device='cuda:1')
epoch:  47000 quantization_loss:  0.01911144144833088
p mean is: tensor(-1.1614, device='cuda:1')
epoch:  48000 quantization_loss:  0.019088681787252426
p mean is: tensor(-1.1686, device='cuda:1')
epoch:  49000 quantization_loss:  0.019065557047724724
p mean is: tensor(-1.1759, device='cuda:1')
epoch:  50000 quantization_loss:  0.019027994945645332
p mean is: tensor(-1.1826, device='cuda:1')
epoch:  51000 quantization_loss:  0.019070254638791084
p mean is: tensor(-1.1885, device='cuda:1')
epoch:  52000 quantization_loss:  0.01912655308842659
p mean is: tensor(-1.1937, device='cuda:1')
epoch:  53000 quantization_loss:  0.01900647208094597
p mean is: tensor(-1.1990, device='cuda:1')
epoch:  54000 quantization_loss:  0.018963288515806198
p mean is: tensor(-1.2034, device='cuda:1')
epoch:  55000 quantization_loss:  0.0189580749720335
p mean is: tensor(-1.2077, device='cuda:1')
epoch:  56000 quantization_loss:  0.018950950354337692
p mean is: tensor(-1.2114, device='cuda:1')
epoch:  57000 quantization_loss:  0.01894158311188221
p mean is: tensor(-1.2154, device='cuda:1')
epoch:  58000 quantization_loss:  0.01891126111149788
p mean is: tensor(-1.2185, device='cuda:1')
epoch:  59000 quantization_loss:  0.018914222717285156
p mean is: tensor(-1.2215, device='cuda:1')
epoch:  60000 quantization_loss:  0.018900981172919273
p mean is: tensor(-1.2248, device='cuda:1')
epoch:  61000 quantization_loss:  0.018872126936912537
p mean is: tensor(-1.2269, device='cuda:1')
epoch:  62000 quantization_loss:  0.018867988139390945
p mean is: tensor(-1.2294, device='cuda:1')
epoch:  63000 quantization_loss:  0.01886403188109398
p mean is: tensor(-1.2318, device='cuda:1')
epoch:  64000 quantization_loss:  0.018866878002882004
p mean is: tensor(-1.2346, device='cuda:1')
epoch:  65000 quantization_loss:  0.018843304365873337
p mean is: tensor(-1.2359, device='cuda:1')
epoch:  66000 quantization_loss:  0.018861090764403343
p mean is: tensor(-1.2379, device='cuda:1')
epoch:  67000 quantization_loss:  0.01884191669523716
p mean is: tensor(-1.2397, device='cuda:1')
epoch:  68000 quantization_loss:  0.018872063606977463
p mean is: tensor(-1.2414, device='cuda:1')
epoch:  69000 quantization_loss:  0.018826840445399284
p mean is: tensor(-1.2428, device='cuda:1')
epoch:  70000 quantization_loss:  0.01882665604352951
p mean is: tensor(-1.2437, device='cuda:1')
epoch:  71000 quantization_loss:  0.01882850006222725
p mean is: tensor(-1.2450, device='cuda:1')
epoch:  72000 quantization_loss:  0.018840748816728592
p mean is: tensor(-1.2460, device='cuda:1')
epoch:  73000 quantization_loss:  0.018810732290148735
p mean is: tensor(-1.2471, device='cuda:1')
epoch:  74000 quantization_loss:  0.01880883052945137
p mean is: tensor(-1.2480, device='cuda:1')
epoch:  75000 quantization_loss:  0.018802015110850334
p mean is: tensor(-1.2489, device='cuda:1')
epoch:  76000 quantization_loss:  0.018809042870998383
p mean is: tensor(-1.2499, device='cuda:1')
epoch:  77000 quantization_loss:  0.018830465152859688
p mean is: tensor(-1.2512, device='cuda:1')
epoch:  78000 quantization_loss:  0.0188442375510931
p mean is: tensor(-1.2517, device='cuda:1')
epoch:  79000 quantization_loss:  0.01879839040338993
p mean is: tensor(-1.2529, device='cuda:1')
here
1.1.weight           | nonzeros =    5462 /   16384             ( 33.34%) | total_pruned =   10922 | shape = torch.Size([128, 128, 1, 1])
4.weight             | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
4.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
5.1.weight           | nonzeros =    4100 /   16384             ( 25.02%) | total_pruned =   12284 | shape = torch.Size([128, 128, 1, 1])
8.weight             | nonzeros =      97 /     128             ( 75.78%) | total_pruned =      31 | shape = torch.Size([128])
8.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
9.1.weight           | nonzeros =    4670 /   16384             ( 28.50%) | total_pruned =   11714 | shape = torch.Size([128, 128, 1, 1])
12.weight            | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
12.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
13.1.weight          | nonzeros =    4292 /   16384             ( 26.20%) | total_pruned =   12092 | shape = torch.Size([128, 128, 1, 1])
16.weight            | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
16.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
17.1.weight          | nonzeros =    3330 /   16384             ( 20.32%) | total_pruned =   13054 | shape = torch.Size([128, 128, 1, 1])
20.weight            | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
20.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
21.1.weight          | nonzeros =    3046 /   16384             ( 18.59%) | total_pruned =   13338 | shape = torch.Size([128, 128, 1, 1])
23.weight            | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
23.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
24.1.weight          | nonzeros =     185 /     384             ( 48.18%) | total_pruned =     199 | shape = torch.Size([3, 128, 1, 1])
alive: 25623, pruned : 74601, total: 100224, Compression rate :       3.91x  ( 74.43% pruned)
PSNR of output image is:  20.236453687090403
Experiment done
