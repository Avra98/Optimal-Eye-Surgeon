(3, 512, 512)
Starting vanilla DIP on 8 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '21.650712047620267'
[128, 128, 128, 128, 128]
Number of params in decoder: 100224
torch.Size([1, 128, 16, 16])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/decoder_mask/8/SAM(sigma=0.1,lr=0.01,decay=0,beta=0)/det/-1.3/1e-09
epoch:  0 quantization_loss:  0.16257326304912567
p mean is: tensor(-0.0002, device='cuda:4')
epoch:  1000 quantization_loss:  0.15899896621704102
p mean is: tensor(-0.0089, device='cuda:4')
epoch:  2000 quantization_loss:  0.13264882564544678
p mean is: tensor(-0.0053, device='cuda:4')
epoch:  3000 quantization_loss:  0.1022319346666336
p mean is: tensor(-0.0060, device='cuda:4')
epoch:  4000 quantization_loss:  0.08457022905349731
p mean is: tensor(-0.0113, device='cuda:4')
epoch:  5000 quantization_loss:  0.07510378211736679
p mean is: tensor(-0.0171, device='cuda:4')
epoch:  6000 quantization_loss:  0.06607821583747864
p mean is: tensor(-0.0243, device='cuda:4')
epoch:  7000 quantization_loss:  0.060000818222761154
p mean is: tensor(-0.0342, device='cuda:4')
epoch:  8000 quantization_loss:  0.05690985172986984
p mean is: tensor(-0.0451, device='cuda:4')
epoch:  9000 quantization_loss:  0.05442129448056221
p mean is: tensor(-0.0568, device='cuda:4')
epoch:  10000 quantization_loss:  0.051917385309934616
p mean is: tensor(-0.0700, device='cuda:4')
epoch:  11000 quantization_loss:  0.052830420434474945
p mean is: tensor(-0.0862, device='cuda:4')
epoch:  12000 quantization_loss:  0.04835766181349754
p mean is: tensor(-0.1052, device='cuda:4')
epoch:  13000 quantization_loss:  0.04786096140742302
p mean is: tensor(-0.1276, device='cuda:4')
epoch:  14000 quantization_loss:  0.04666455462574959
p mean is: tensor(-0.1543, device='cuda:4')
epoch:  15000 quantization_loss:  0.046245280653238297
p mean is: tensor(-0.1836, device='cuda:4')
epoch:  16000 quantization_loss:  0.04445616155862808
p mean is: tensor(-0.2156, device='cuda:4')
epoch:  17000 quantization_loss:  0.04351602494716644
p mean is: tensor(-0.2495, device='cuda:4')
epoch:  18000 quantization_loss:  0.04302564635872841
p mean is: tensor(-0.2835, device='cuda:4')
epoch:  19000 quantization_loss:  0.04231021925806999
p mean is: tensor(-0.3164, device='cuda:4')
epoch:  20000 quantization_loss:  0.0418708398938179
p mean is: tensor(-0.3483, device='cuda:4')
epoch:  21000 quantization_loss:  0.04082370176911354
p mean is: tensor(-0.3794, device='cuda:4')
epoch:  22000 quantization_loss:  0.04031740874052048
p mean is: tensor(-0.4090, device='cuda:4')
epoch:  23000 quantization_loss:  0.04024427756667137
p mean is: tensor(-0.4368, device='cuda:4')
epoch:  24000 quantization_loss:  0.04004417359828949
p mean is: tensor(-0.4633, device='cuda:4')
epoch:  25000 quantization_loss:  0.03948891907930374
p mean is: tensor(-0.4890, device='cuda:4')
epoch:  26000 quantization_loss:  0.03929508477449417
p mean is: tensor(-0.5111, device='cuda:4')
epoch:  27000 quantization_loss:  0.038763053715229034
p mean is: tensor(-0.5335, device='cuda:4')
epoch:  28000 quantization_loss:  0.038424406200647354
p mean is: tensor(-0.5537, device='cuda:4')
epoch:  29000 quantization_loss:  0.038370564579963684
p mean is: tensor(-0.5736, device='cuda:4')
epoch:  30000 quantization_loss:  0.03805618733167648
p mean is: tensor(-0.5925, device='cuda:4')
epoch:  31000 quantization_loss:  0.03795216977596283
p mean is: tensor(-0.6109, device='cuda:4')
epoch:  32000 quantization_loss:  0.037932734936475754
p mean is: tensor(-0.6277, device='cuda:4')
epoch:  33000 quantization_loss:  0.037679895758628845
p mean is: tensor(-0.6424, device='cuda:4')
epoch:  34000 quantization_loss:  0.037655096501111984
p mean is: tensor(-0.6560, device='cuda:4')
epoch:  35000 quantization_loss:  0.03747888654470444
p mean is: tensor(-0.6680, device='cuda:4')
epoch:  36000 quantization_loss:  0.0374879352748394
p mean is: tensor(-0.6791, device='cuda:4')
epoch:  37000 quantization_loss:  0.03745899721980095
p mean is: tensor(-0.6888, device='cuda:4')
epoch:  38000 quantization_loss:  0.03719519078731537
p mean is: tensor(-0.6981, device='cuda:4')
epoch:  39000 quantization_loss:  0.037226323038339615
p mean is: tensor(-0.7066, device='cuda:4')
epoch:  40000 quantization_loss:  0.0371321402490139
p mean is: tensor(-0.7137, device='cuda:4')
epoch:  41000 quantization_loss:  0.03709326684474945
p mean is: tensor(-0.7195, device='cuda:4')
epoch:  42000 quantization_loss:  0.037093162536621094
p mean is: tensor(-0.7261, device='cuda:4')
epoch:  43000 quantization_loss:  0.03701530769467354
p mean is: tensor(-0.7315, device='cuda:4')
epoch:  44000 quantization_loss:  0.03696989640593529
p mean is: tensor(-0.7357, device='cuda:4')
epoch:  45000 quantization_loss:  0.03700118884444237
p mean is: tensor(-0.7390, device='cuda:4')
epoch:  46000 quantization_loss:  0.03688937798142433
p mean is: tensor(-0.7417, device='cuda:4')
epoch:  47000 quantization_loss:  0.036810606718063354
p mean is: tensor(-0.7444, device='cuda:4')
epoch:  48000 quantization_loss:  0.03683144599199295
p mean is: tensor(-0.7466, device='cuda:4')
epoch:  49000 quantization_loss:  0.03680882975459099
p mean is: tensor(-0.7485, device='cuda:4')
epoch:  50000 quantization_loss:  0.03676034137606621
p mean is: tensor(-0.7503, device='cuda:4')
epoch:  51000 quantization_loss:  0.03670615330338478
p mean is: tensor(-0.7518, device='cuda:4')
epoch:  52000 quantization_loss:  0.0367560051381588
p mean is: tensor(-0.7529, device='cuda:4')
epoch:  53000 quantization_loss:  0.036742981523275375
p mean is: tensor(-0.7539, device='cuda:4')
epoch:  54000 quantization_loss:  0.03667116537690163
p mean is: tensor(-0.7545, device='cuda:4')
epoch:  55000 quantization_loss:  0.03662511333823204
p mean is: tensor(-0.7549, device='cuda:4')
epoch:  56000 quantization_loss:  0.03661566972732544
p mean is: tensor(-0.7561, device='cuda:4')
epoch:  57000 quantization_loss:  0.0365816093981266
p mean is: tensor(-0.7572, device='cuda:4')
epoch:  58000 quantization_loss:  0.03660018742084503
p mean is: tensor(-0.7577, device='cuda:4')
epoch:  59000 quantization_loss:  0.03656993433833122
p mean is: tensor(-0.7580, device='cuda:4')
epoch:  60000 quantization_loss:  0.036544106900691986
p mean is: tensor(-0.7577, device='cuda:4')
epoch:  61000 quantization_loss:  0.03655073419213295
p mean is: tensor(-0.7581, device='cuda:4')
epoch:  62000 quantization_loss:  0.03654451668262482
p mean is: tensor(-0.7587, device='cuda:4')
epoch:  63000 quantization_loss:  0.03653721511363983
p mean is: tensor(-0.7584, device='cuda:4')
epoch:  64000 quantization_loss:  0.036534037441015244
p mean is: tensor(-0.7579, device='cuda:4')
epoch:  65000 quantization_loss:  0.0365019217133522
p mean is: tensor(-0.7574, device='cuda:4')
epoch:  66000 quantization_loss:  0.03652585297822952
p mean is: tensor(-0.7574, device='cuda:4')
epoch:  67000 quantization_loss:  0.036592867225408554
p mean is: tensor(-0.7576, device='cuda:4')
epoch:  68000 quantization_loss:  0.03648127242922783
p mean is: tensor(-0.7579, device='cuda:4')
epoch:  69000 quantization_loss:  0.03649187833070755
p mean is: tensor(-0.7578, device='cuda:4')
epoch:  70000 quantization_loss:  0.03646117076277733
p mean is: tensor(-0.7577, device='cuda:4')
epoch:  71000 quantization_loss:  0.036449532955884933
p mean is: tensor(-0.7575, device='cuda:4')
epoch:  72000 quantization_loss:  0.03644448518753052
p mean is: tensor(-0.7572, device='cuda:4')
epoch:  73000 quantization_loss:  0.03646538034081459
p mean is: tensor(-0.7571, device='cuda:4')
epoch:  74000 quantization_loss:  0.03643343225121498
p mean is: tensor(-0.7565, device='cuda:4')
epoch:  75000 quantization_loss:  0.036500584334135056
p mean is: tensor(-0.7558, device='cuda:4')
epoch:  76000 quantization_loss:  0.03643910214304924
p mean is: tensor(-0.7552, device='cuda:4')
epoch:  77000 quantization_loss:  0.03647667542099953
p mean is: tensor(-0.7553, device='cuda:4')
epoch:  78000 quantization_loss:  0.03644578903913498
p mean is: tensor(-0.7552, device='cuda:4')
epoch:  79000 quantization_loss:  0.036419060081243515
p mean is: tensor(-0.7549, device='cuda:4')
here
1.1.weight           | nonzeros =    5271 /   16384             ( 32.17%) | total_pruned =   11113 | shape = torch.Size([128, 128, 1, 1])
4.weight             | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
4.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
5.1.weight           | nonzeros =    4023 /   16384             ( 24.55%) | total_pruned =   12361 | shape = torch.Size([128, 128, 1, 1])
8.weight             | nonzeros =      99 /     128             ( 77.34%) | total_pruned =      29 | shape = torch.Size([128])
8.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
9.1.weight           | nonzeros =    3906 /   16384             ( 23.84%) | total_pruned =   12478 | shape = torch.Size([128, 128, 1, 1])
12.weight            | nonzeros =      77 /     128             ( 60.16%) | total_pruned =      51 | shape = torch.Size([128])
12.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
13.1.weight          | nonzeros =    1827 /   16384             ( 11.15%) | total_pruned =   14557 | shape = torch.Size([128, 128, 1, 1])
16.weight            | nonzeros =      49 /     128             ( 38.28%) | total_pruned =      79 | shape = torch.Size([128])
16.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
17.1.weight          | nonzeros =     908 /   16384             (  5.54%) | total_pruned =   15476 | shape = torch.Size([128, 128, 1, 1])
20.weight            | nonzeros =      39 /     128             ( 30.47%) | total_pruned =      89 | shape = torch.Size([128])
20.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
21.1.weight          | nonzeros =    2351 /   16384             ( 14.35%) | total_pruned =   14033 | shape = torch.Size([128, 128, 1, 1])
23.weight            | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
23.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
24.1.weight          | nonzeros =     267 /     384             ( 69.53%) | total_pruned =     117 | shape = torch.Size([3, 128, 1, 1])
alive: 19023, pruned : 81201, total: 100224, Compression rate :       5.27x  ( 81.02% pruned)
PSNR of output image is:  14.507855126090458
Experiment done
