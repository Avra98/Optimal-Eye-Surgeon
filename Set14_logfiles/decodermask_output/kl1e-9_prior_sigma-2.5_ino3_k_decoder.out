(3, 256, 256)
Starting vanilla DIP on 3 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '20.304808476646244'
[128, 128, 128, 128, 128]
Number of params in decoder: 100224
torch.Size([1, 128, 8, 8])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/decoder_mask/3/SAM(sigma=0.1,lr=0.01,decay=0,beta=0)/det/-2.5/1e-09
epoch:  0 quantization_loss:  0.07528365403413773
p mean is: tensor(-0.0004, device='cuda:4')
epoch:  1000 quantization_loss:  0.07475550472736359
p mean is: tensor(-0.0122, device='cuda:4')
epoch:  2000 quantization_loss:  0.07294469326734543
p mean is: tensor(-0.0197, device='cuda:4')
epoch:  3000 quantization_loss:  0.062285467982292175
p mean is: tensor(-0.0221, device='cuda:4')
epoch:  4000 quantization_loss:  0.05595288798213005
p mean is: tensor(-0.0251, device='cuda:4')
epoch:  5000 quantization_loss:  0.05131414532661438
p mean is: tensor(-0.0285, device='cuda:4')
epoch:  6000 quantization_loss:  0.04745824635028839
p mean is: tensor(-0.0338, device='cuda:4')
epoch:  7000 quantization_loss:  0.04510701075196266
p mean is: tensor(-0.0408, device='cuda:4')
epoch:  8000 quantization_loss:  0.04264787584543228
p mean is: tensor(-0.0483, device='cuda:4')
epoch:  9000 quantization_loss:  0.0417761392891407
p mean is: tensor(-0.0582, device='cuda:4')
epoch:  10000 quantization_loss:  0.039737507700920105
p mean is: tensor(-0.0697, device='cuda:4')
epoch:  11000 quantization_loss:  0.038271378725767136
p mean is: tensor(-0.0853, device='cuda:4')
epoch:  12000 quantization_loss:  0.0379999615252018
p mean is: tensor(-0.1038, device='cuda:4')
epoch:  13000 quantization_loss:  0.03674302622675896
p mean is: tensor(-0.1240, device='cuda:4')
epoch:  14000 quantization_loss:  0.036124031990766525
p mean is: tensor(-0.1495, device='cuda:4')
epoch:  15000 quantization_loss:  0.0365523062646389
p mean is: tensor(-0.1779, device='cuda:4')
epoch:  16000 quantization_loss:  0.03462141007184982
p mean is: tensor(-0.2092, device='cuda:4')
epoch:  17000 quantization_loss:  0.03433777764439583
p mean is: tensor(-0.2427, device='cuda:4')
epoch:  18000 quantization_loss:  0.03418640047311783
p mean is: tensor(-0.2792, device='cuda:4')
epoch:  19000 quantization_loss:  0.0327235609292984
p mean is: tensor(-0.3162, device='cuda:4')
epoch:  20000 quantization_loss:  0.03280381113290787
p mean is: tensor(-0.3544, device='cuda:4')
epoch:  21000 quantization_loss:  0.03179335966706276
p mean is: tensor(-0.3939, device='cuda:4')
epoch:  22000 quantization_loss:  0.03166995942592621
p mean is: tensor(-0.4340, device='cuda:4')
epoch:  23000 quantization_loss:  0.03128458559513092
p mean is: tensor(-0.4732, device='cuda:4')
epoch:  24000 quantization_loss:  0.030857399106025696
p mean is: tensor(-0.5107, device='cuda:4')
epoch:  25000 quantization_loss:  0.0304779764264822
p mean is: tensor(-0.5479, device='cuda:4')
epoch:  26000 quantization_loss:  0.03026212565600872
p mean is: tensor(-0.5842, device='cuda:4')
epoch:  27000 quantization_loss:  0.029855947941541672
p mean is: tensor(-0.6199, device='cuda:4')
epoch:  28000 quantization_loss:  0.02957949787378311
p mean is: tensor(-0.6550, device='cuda:4')
epoch:  29000 quantization_loss:  0.029698889702558517
p mean is: tensor(-0.6879, device='cuda:4')
epoch:  30000 quantization_loss:  0.02921878919005394
p mean is: tensor(-0.7197, device='cuda:4')
epoch:  31000 quantization_loss:  0.0290877316147089
p mean is: tensor(-0.7515, device='cuda:4')
epoch:  32000 quantization_loss:  0.029075413942337036
p mean is: tensor(-0.7818, device='cuda:4')
epoch:  33000 quantization_loss:  0.02871156670153141
p mean is: tensor(-0.8095, device='cuda:4')
epoch:  34000 quantization_loss:  0.02849273569881916
p mean is: tensor(-0.8353, device='cuda:4')
epoch:  35000 quantization_loss:  0.028548408299684525
p mean is: tensor(-0.8594, device='cuda:4')
epoch:  36000 quantization_loss:  0.02845780923962593
p mean is: tensor(-0.8812, device='cuda:4')
epoch:  37000 quantization_loss:  0.028198255226016045
p mean is: tensor(-0.9013, device='cuda:4')
epoch:  38000 quantization_loss:  0.028196077793836594
p mean is: tensor(-0.9200, device='cuda:4')
epoch:  39000 quantization_loss:  0.028107548132538795
p mean is: tensor(-0.9363, device='cuda:4')
epoch:  40000 quantization_loss:  0.02799445576965809
p mean is: tensor(-0.9512, device='cuda:4')
epoch:  41000 quantization_loss:  0.027931125834584236
p mean is: tensor(-0.9645, device='cuda:4')
epoch:  42000 quantization_loss:  0.02789381332695484
p mean is: tensor(-0.9784, device='cuda:4')
epoch:  43000 quantization_loss:  0.02787173166871071
p mean is: tensor(-0.9902, device='cuda:4')
epoch:  44000 quantization_loss:  0.027853544801473618
p mean is: tensor(-1.0021, device='cuda:4')
epoch:  45000 quantization_loss:  0.027797169983386993
p mean is: tensor(-1.0124, device='cuda:4')
epoch:  46000 quantization_loss:  0.028353378176689148
p mean is: tensor(-1.0219, device='cuda:4')
epoch:  47000 quantization_loss:  0.027691055089235306
p mean is: tensor(-1.0302, device='cuda:4')
epoch:  48000 quantization_loss:  0.02770710363984108
p mean is: tensor(-1.0381, device='cuda:4')
epoch:  49000 quantization_loss:  0.027637850493192673
p mean is: tensor(-1.0445, device='cuda:4')
epoch:  50000 quantization_loss:  0.027625320479273796
p mean is: tensor(-1.0501, device='cuda:4')
epoch:  51000 quantization_loss:  0.027618741616606712
p mean is: tensor(-1.0562, device='cuda:4')
epoch:  52000 quantization_loss:  0.027586303651332855
p mean is: tensor(-1.0620, device='cuda:4')
epoch:  53000 quantization_loss:  0.027553601190447807
p mean is: tensor(-1.0673, device='cuda:4')
epoch:  54000 quantization_loss:  0.02757197432219982
p mean is: tensor(-1.0711, device='cuda:4')
epoch:  55000 quantization_loss:  0.027556998655200005
p mean is: tensor(-1.0752, device='cuda:4')
epoch:  56000 quantization_loss:  0.028039203956723213
p mean is: tensor(-1.0789, device='cuda:4')
epoch:  57000 quantization_loss:  0.02748854272067547
p mean is: tensor(-1.0829, device='cuda:4')
epoch:  58000 quantization_loss:  0.02750042825937271
p mean is: tensor(-1.0865, device='cuda:4')
epoch:  59000 quantization_loss:  0.027500469237565994
p mean is: tensor(-1.0898, device='cuda:4')
epoch:  60000 quantization_loss:  0.02745172381401062
p mean is: tensor(-1.0933, device='cuda:4')
epoch:  61000 quantization_loss:  0.0274448711425066
p mean is: tensor(-1.0957, device='cuda:4')
epoch:  62000 quantization_loss:  0.027460385113954544
p mean is: tensor(-1.0987, device='cuda:4')
epoch:  63000 quantization_loss:  0.02744135446846485
p mean is: tensor(-1.1008, device='cuda:4')
epoch:  64000 quantization_loss:  0.027404284104704857
p mean is: tensor(-1.1034, device='cuda:4')
epoch:  65000 quantization_loss:  0.027440233156085014
p mean is: tensor(-1.1055, device='cuda:4')
epoch:  66000 quantization_loss:  0.027436284348368645
p mean is: tensor(-1.1076, device='cuda:4')
epoch:  67000 quantization_loss:  0.02739856392145157
p mean is: tensor(-1.1092, device='cuda:4')
epoch:  68000 quantization_loss:  0.027366939932107925
p mean is: tensor(-1.1112, device='cuda:4')
epoch:  69000 quantization_loss:  0.027387386187911034
p mean is: tensor(-1.1128, device='cuda:4')
epoch:  70000 quantization_loss:  0.027360333129763603
p mean is: tensor(-1.1145, device='cuda:4')
epoch:  71000 quantization_loss:  0.027372512966394424
p mean is: tensor(-1.1157, device='cuda:4')
epoch:  72000 quantization_loss:  0.02736176736652851
p mean is: tensor(-1.1166, device='cuda:4')
epoch:  73000 quantization_loss:  0.02736109308898449
p mean is: tensor(-1.1171, device='cuda:4')
epoch:  74000 quantization_loss:  0.027432838454842567
p mean is: tensor(-1.1181, device='cuda:4')
epoch:  75000 quantization_loss:  0.027364570647478104
p mean is: tensor(-1.1184, device='cuda:4')
epoch:  76000 quantization_loss:  0.02735210210084915
p mean is: tensor(-1.1189, device='cuda:4')
epoch:  77000 quantization_loss:  0.027351846918463707
p mean is: tensor(-1.1196, device='cuda:4')
epoch:  78000 quantization_loss:  0.02733560837805271
p mean is: tensor(-1.1203, device='cuda:4')
epoch:  79000 quantization_loss:  0.027324175462126732
p mean is: tensor(-1.1214, device='cuda:4')
here
1.1.weight           | nonzeros =    5032 /   16384             ( 30.71%) | total_pruned =   11352 | shape = torch.Size([128, 128, 1, 1])
4.weight             | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
4.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
5.1.weight           | nonzeros =    4061 /   16384             ( 24.79%) | total_pruned =   12323 | shape = torch.Size([128, 128, 1, 1])
8.weight             | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
8.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
9.1.weight           | nonzeros =    5142 /   16384             ( 31.38%) | total_pruned =   11242 | shape = torch.Size([128, 128, 1, 1])
12.weight            | nonzeros =      99 /     128             ( 77.34%) | total_pruned =      29 | shape = torch.Size([128])
12.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
13.1.weight          | nonzeros =    4688 /   16384             ( 28.61%) | total_pruned =   11696 | shape = torch.Size([128, 128, 1, 1])
16.weight            | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
16.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
17.1.weight          | nonzeros =    3893 /   16384             ( 23.76%) | total_pruned =   12491 | shape = torch.Size([128, 128, 1, 1])
20.weight            | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
20.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
21.1.weight          | nonzeros =    3096 /   16384             ( 18.90%) | total_pruned =   13288 | shape = torch.Size([128, 128, 1, 1])
23.weight            | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
23.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
24.1.weight          | nonzeros =     152 /     384             ( 39.58%) | total_pruned =     232 | shape = torch.Size([3, 128, 1, 1])
alive: 26602, pruned : 73622, total: 100224, Compression rate :       3.77x  ( 73.46% pruned)
PSNR of output image is:  17.162232553036738
Experiment done
