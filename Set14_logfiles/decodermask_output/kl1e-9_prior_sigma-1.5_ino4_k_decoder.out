(3, 512, 512)
Starting vanilla DIP on 4 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '19.7559511537822'
[128, 128, 128, 128, 128]
Number of params in decoder: 100224
torch.Size([1, 128, 16, 16])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/decoder_mask/4/SAM(sigma=0.1,lr=0.01,decay=0,beta=0)/det/-1.5/1e-09
epoch:  0 quantization_loss:  0.06706525385379791
p mean is: tensor(-0.0002, device='cuda:1')
epoch:  1000 quantization_loss:  0.0630536824464798
p mean is: tensor(-0.0119, device='cuda:1')
epoch:  2000 quantization_loss:  0.06307349354028702
p mean is: tensor(-0.0203, device='cuda:1')
epoch:  3000 quantization_loss:  0.06262227892875671
p mean is: tensor(-0.0287, device='cuda:1')
epoch:  4000 quantization_loss:  0.05345288664102554
p mean is: tensor(-0.0330, device='cuda:1')
epoch:  5000 quantization_loss:  0.04192833974957466
p mean is: tensor(-0.0375, device='cuda:1')
epoch:  6000 quantization_loss:  0.037594862282276154
p mean is: tensor(-0.0466, device='cuda:1')
epoch:  7000 quantization_loss:  0.03440818190574646
p mean is: tensor(-0.0559, device='cuda:1')
epoch:  8000 quantization_loss:  0.03343245014548302
p mean is: tensor(-0.0689, device='cuda:1')
epoch:  9000 quantization_loss:  0.0317087285220623
p mean is: tensor(-0.0837, device='cuda:1')
epoch:  10000 quantization_loss:  0.029368672519922256
p mean is: tensor(-0.1011, device='cuda:1')
epoch:  11000 quantization_loss:  0.028540030121803284
p mean is: tensor(-0.1193, device='cuda:1')
epoch:  12000 quantization_loss:  0.027383845299482346
p mean is: tensor(-0.1414, device='cuda:1')
epoch:  13000 quantization_loss:  0.026588749140501022
p mean is: tensor(-0.1666, device='cuda:1')
epoch:  14000 quantization_loss:  0.02590583823621273
p mean is: tensor(-0.1939, device='cuda:1')
epoch:  15000 quantization_loss:  0.025493593886494637
p mean is: tensor(-0.2233, device='cuda:1')
epoch:  16000 quantization_loss:  0.02484559640288353
p mean is: tensor(-0.2546, device='cuda:1')
epoch:  17000 quantization_loss:  0.02470500022172928
p mean is: tensor(-0.2853, device='cuda:1')
epoch:  18000 quantization_loss:  0.024377113208174706
p mean is: tensor(-0.3173, device='cuda:1')
epoch:  19000 quantization_loss:  0.02377753145992756
p mean is: tensor(-0.3490, device='cuda:1')
epoch:  20000 quantization_loss:  0.023739218711853027
p mean is: tensor(-0.3800, device='cuda:1')
epoch:  21000 quantization_loss:  0.023406436666846275
p mean is: tensor(-0.4103, device='cuda:1')
epoch:  22000 quantization_loss:  0.023300370201468468
p mean is: tensor(-0.4390, device='cuda:1')
epoch:  23000 quantization_loss:  0.022915741428732872
p mean is: tensor(-0.4659, device='cuda:1')
epoch:  24000 quantization_loss:  0.02269858494400978
p mean is: tensor(-0.4914, device='cuda:1')
epoch:  25000 quantization_loss:  0.022475866600871086
p mean is: tensor(-0.5168, device='cuda:1')
epoch:  26000 quantization_loss:  0.022374192252755165
p mean is: tensor(-0.5401, device='cuda:1')
epoch:  27000 quantization_loss:  0.022237032651901245
p mean is: tensor(-0.5624, device='cuda:1')
epoch:  28000 quantization_loss:  0.022135768085718155
p mean is: tensor(-0.5826, device='cuda:1')
epoch:  29000 quantization_loss:  0.022094963118433952
p mean is: tensor(-0.6018, device='cuda:1')
epoch:  30000 quantization_loss:  0.022010255604982376
p mean is: tensor(-0.6186, device='cuda:1')
epoch:  31000 quantization_loss:  0.021912407130002975
p mean is: tensor(-0.6341, device='cuda:1')
epoch:  32000 quantization_loss:  0.0218791626393795
p mean is: tensor(-0.6475, device='cuda:1')
epoch:  33000 quantization_loss:  0.021775443106889725
p mean is: tensor(-0.6604, device='cuda:1')
epoch:  34000 quantization_loss:  0.021601995453238487
p mean is: tensor(-0.6723, device='cuda:1')
epoch:  35000 quantization_loss:  0.0214324202388525
p mean is: tensor(-0.6814, device='cuda:1')
epoch:  36000 quantization_loss:  0.02135942503809929
p mean is: tensor(-0.6898, device='cuda:1')
epoch:  37000 quantization_loss:  0.021299388259649277
p mean is: tensor(-0.6987, device='cuda:1')
epoch:  38000 quantization_loss:  0.02112201228737831
p mean is: tensor(-0.7061, device='cuda:1')
epoch:  39000 quantization_loss:  0.021160442382097244
p mean is: tensor(-0.7131, device='cuda:1')
epoch:  40000 quantization_loss:  0.021093135699629784
p mean is: tensor(-0.7188, device='cuda:1')
epoch:  41000 quantization_loss:  0.02114221081137657
p mean is: tensor(-0.7251, device='cuda:1')
epoch:  42000 quantization_loss:  0.021005704998970032
p mean is: tensor(-0.7308, device='cuda:1')
epoch:  43000 quantization_loss:  0.020938923582434654
p mean is: tensor(-0.7353, device='cuda:1')
epoch:  44000 quantization_loss:  0.02090347371995449
p mean is: tensor(-0.7390, device='cuda:1')
epoch:  45000 quantization_loss:  0.02093617618083954
p mean is: tensor(-0.7435, device='cuda:1')
epoch:  46000 quantization_loss:  0.02095349319279194
p mean is: tensor(-0.7469, device='cuda:1')
epoch:  47000 quantization_loss:  0.020832467824220657
p mean is: tensor(-0.7502, device='cuda:1')
epoch:  48000 quantization_loss:  0.020905500277876854
p mean is: tensor(-0.7533, device='cuda:1')
epoch:  49000 quantization_loss:  0.020833352580666542
p mean is: tensor(-0.7571, device='cuda:1')
epoch:  50000 quantization_loss:  0.020812399685382843
p mean is: tensor(-0.7602, device='cuda:1')
epoch:  51000 quantization_loss:  0.02078663930296898
p mean is: tensor(-0.7635, device='cuda:1')
epoch:  52000 quantization_loss:  0.020837796851992607
p mean is: tensor(-0.7659, device='cuda:1')
epoch:  53000 quantization_loss:  0.0207983311265707
p mean is: tensor(-0.7681, device='cuda:1')
epoch:  54000 quantization_loss:  0.020773742347955704
p mean is: tensor(-0.7700, device='cuda:1')
epoch:  55000 quantization_loss:  0.020752718672156334
p mean is: tensor(-0.7719, device='cuda:1')
epoch:  56000 quantization_loss:  0.020741576328873634
p mean is: tensor(-0.7729, device='cuda:1')
epoch:  57000 quantization_loss:  0.020724041387438774
p mean is: tensor(-0.7742, device='cuda:1')
epoch:  58000 quantization_loss:  0.020715171471238136
p mean is: tensor(-0.7754, device='cuda:1')
epoch:  59000 quantization_loss:  0.020709015429019928
p mean is: tensor(-0.7767, device='cuda:1')
epoch:  60000 quantization_loss:  0.020717130973935127
p mean is: tensor(-0.7783, device='cuda:1')
epoch:  61000 quantization_loss:  0.020751362666487694
p mean is: tensor(-0.7796, device='cuda:1')
epoch:  62000 quantization_loss:  0.020686384290456772
p mean is: tensor(-0.7803, device='cuda:1')
epoch:  63000 quantization_loss:  0.020679783076047897
p mean is: tensor(-0.7813, device='cuda:1')
epoch:  64000 quantization_loss:  0.02072312869131565
p mean is: tensor(-0.7817, device='cuda:1')
epoch:  65000 quantization_loss:  0.02066958136856556
p mean is: tensor(-0.7821, device='cuda:1')
epoch:  66000 quantization_loss:  0.0206645205616951
p mean is: tensor(-0.7822, device='cuda:1')
epoch:  67000 quantization_loss:  0.02085067331790924
p mean is: tensor(-0.7828, device='cuda:1')
epoch:  68000 quantization_loss:  0.0206841342151165
p mean is: tensor(-0.7836, device='cuda:1')
epoch:  69000 quantization_loss:  0.020658448338508606
p mean is: tensor(-0.7843, device='cuda:1')
epoch:  70000 quantization_loss:  0.020660657435655594
p mean is: tensor(-0.7851, device='cuda:1')
epoch:  71000 quantization_loss:  0.02065999060869217
p mean is: tensor(-0.7859, device='cuda:1')
epoch:  72000 quantization_loss:  0.020650772377848625
p mean is: tensor(-0.7864, device='cuda:1')
epoch:  73000 quantization_loss:  0.020669003948569298
p mean is: tensor(-0.7874, device='cuda:1')
epoch:  74000 quantization_loss:  0.02065124735236168
p mean is: tensor(-0.7886, device='cuda:1')
epoch:  75000 quantization_loss:  0.020635848864912987
p mean is: tensor(-0.7896, device='cuda:1')
epoch:  76000 quantization_loss:  0.02063753828406334
p mean is: tensor(-0.7898, device='cuda:1')
epoch:  77000 quantization_loss:  0.02058914676308632
p mean is: tensor(-0.7902, device='cuda:1')
epoch:  78000 quantization_loss:  0.02058153972029686
p mean is: tensor(-0.7904, device='cuda:1')
epoch:  79000 quantization_loss:  0.02058606967329979
p mean is: tensor(-0.7911, device='cuda:1')
here
1.1.weight           | nonzeros =    4980 /   16384             ( 30.40%) | total_pruned =   11404 | shape = torch.Size([128, 128, 1, 1])
4.weight             | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
4.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
5.1.weight           | nonzeros =    3842 /   16384             ( 23.45%) | total_pruned =   12542 | shape = torch.Size([128, 128, 1, 1])
8.weight             | nonzeros =      99 /     128             ( 77.34%) | total_pruned =      29 | shape = torch.Size([128])
8.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
9.1.weight           | nonzeros =    4585 /   16384             ( 27.98%) | total_pruned =   11799 | shape = torch.Size([128, 128, 1, 1])
12.weight            | nonzeros =      95 /     128             ( 74.22%) | total_pruned =      33 | shape = torch.Size([128])
12.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
13.1.weight          | nonzeros =    3846 /   16384             ( 23.47%) | total_pruned =   12538 | shape = torch.Size([128, 128, 1, 1])
16.weight            | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
16.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
17.1.weight          | nonzeros =    3345 /   16384             ( 20.42%) | total_pruned =   13039 | shape = torch.Size([128, 128, 1, 1])
20.weight            | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
20.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
21.1.weight          | nonzeros =    4395 /   16384             ( 26.82%) | total_pruned =   11989 | shape = torch.Size([128, 128, 1, 1])
23.weight            | nonzeros =     122 /     128             ( 95.31%) | total_pruned =       6 | shape = torch.Size([128])
23.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
24.1.weight          | nonzeros =     223 /     384             ( 58.07%) | total_pruned =     161 | shape = torch.Size([3, 128, 1, 1])
alive: 25775, pruned : 74449, total: 100224, Compression rate :       3.89x  ( 74.28% pruned)
PSNR of output image is:  18.978593683550397
Experiment done
