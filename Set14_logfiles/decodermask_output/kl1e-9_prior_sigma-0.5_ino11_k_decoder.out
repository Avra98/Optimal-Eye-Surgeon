(3, 512, 512)
Starting vanilla DIP on 11 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '20.308791545667063'
[128, 128, 128, 128, 128]
Number of params in decoder: 100224
torch.Size([1, 128, 16, 16])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/decoder_mask/11/SAM(sigma=0.1,lr=0.01,decay=0,beta=0)/det/-0.5/1e-09
epoch:  0 quantization_loss:  0.06905373185873032
p mean is: tensor(-0.0001, device='cuda:4')
epoch:  1000 quantization_loss:  0.06318660825490952
p mean is: tensor(-0.0077, device='cuda:4')
epoch:  2000 quantization_loss:  0.055282071232795715
p mean is: tensor(-0.0059, device='cuda:4')
epoch:  3000 quantization_loss:  0.049449484795331955
p mean is: tensor(-0.0023, device='cuda:4')
epoch:  4000 quantization_loss:  0.047391343861818314
p mean is: tensor(-0.0008, device='cuda:4')
epoch:  5000 quantization_loss:  0.04639049619436264
p mean is: tensor(-0.0009, device='cuda:4')
epoch:  6000 quantization_loss:  0.04516938701272011
p mean is: tensor(-0.0003, device='cuda:4')
epoch:  7000 quantization_loss:  0.044630780816078186
p mean is: tensor(-0.0006, device='cuda:4')
epoch:  8000 quantization_loss:  0.043009061366319656
p mean is: tensor(-0.0021, device='cuda:4')
epoch:  9000 quantization_loss:  0.041952162981033325
p mean is: tensor(-0.0044, device='cuda:4')
epoch:  10000 quantization_loss:  0.04186857119202614
p mean is: tensor(-0.0081, device='cuda:4')
epoch:  11000 quantization_loss:  0.040452755987644196
p mean is: tensor(-0.0137, device='cuda:4')
epoch:  12000 quantization_loss:  0.03971511870622635
p mean is: tensor(-0.0183, device='cuda:4')
epoch:  13000 quantization_loss:  0.039216019213199615
p mean is: tensor(-0.0257, device='cuda:4')
epoch:  14000 quantization_loss:  0.038608793169260025
p mean is: tensor(-0.0349, device='cuda:4')
epoch:  15000 quantization_loss:  0.03828621283173561
p mean is: tensor(-0.0446, device='cuda:4')
epoch:  16000 quantization_loss:  0.037107955664396286
p mean is: tensor(-0.0542, device='cuda:4')
epoch:  17000 quantization_loss:  0.036961302161216736
p mean is: tensor(-0.0648, device='cuda:4')
epoch:  18000 quantization_loss:  0.035620514303445816
p mean is: tensor(-0.0769, device='cuda:4')
epoch:  19000 quantization_loss:  0.03501620888710022
p mean is: tensor(-0.0871, device='cuda:4')
epoch:  20000 quantization_loss:  0.03471410274505615
p mean is: tensor(-0.0975, device='cuda:4')
epoch:  21000 quantization_loss:  0.0341620109975338
p mean is: tensor(-0.1063, device='cuda:4')
epoch:  22000 quantization_loss:  0.03369341418147087
p mean is: tensor(-0.1156, device='cuda:4')
epoch:  23000 quantization_loss:  0.0336022712290287
p mean is: tensor(-0.1237, device='cuda:4')
epoch:  24000 quantization_loss:  0.03349926322698593
p mean is: tensor(-0.1306, device='cuda:4')
epoch:  25000 quantization_loss:  0.032961759716272354
p mean is: tensor(-0.1373, device='cuda:4')
epoch:  26000 quantization_loss:  0.03254043310880661
p mean is: tensor(-0.1427, device='cuda:4')
epoch:  27000 quantization_loss:  0.03213733807206154
p mean is: tensor(-0.1493, device='cuda:4')
epoch:  28000 quantization_loss:  0.03183310851454735
p mean is: tensor(-0.1534, device='cuda:4')
epoch:  29000 quantization_loss:  0.0317717008292675
p mean is: tensor(-0.1582, device='cuda:4')
epoch:  30000 quantization_loss:  0.03159694746136665
p mean is: tensor(-0.1626, device='cuda:4')
epoch:  31000 quantization_loss:  0.03145495057106018
p mean is: tensor(-0.1658, device='cuda:4')
epoch:  32000 quantization_loss:  0.031233616173267365
p mean is: tensor(-0.1686, device='cuda:4')
epoch:  33000 quantization_loss:  0.03103799745440483
p mean is: tensor(-0.1710, device='cuda:4')
epoch:  34000 quantization_loss:  0.031007688492536545
p mean is: tensor(-0.1731, device='cuda:4')
epoch:  35000 quantization_loss:  0.030857456848025322
p mean is: tensor(-0.1740, device='cuda:4')
epoch:  36000 quantization_loss:  0.030788255855441093
p mean is: tensor(-0.1752, device='cuda:4')
epoch:  37000 quantization_loss:  0.030776746571063995
p mean is: tensor(-0.1753, device='cuda:4')
epoch:  38000 quantization_loss:  0.03067726641893387
p mean is: tensor(-0.1757, device='cuda:4')
epoch:  39000 quantization_loss:  0.030574416741728783
p mean is: tensor(-0.1760, device='cuda:4')
epoch:  40000 quantization_loss:  0.030477255582809448
p mean is: tensor(-0.1759, device='cuda:4')
epoch:  41000 quantization_loss:  0.03044268861413002
p mean is: tensor(-0.1746, device='cuda:4')
epoch:  42000 quantization_loss:  0.030433354899287224
p mean is: tensor(-0.1733, device='cuda:4')
epoch:  43000 quantization_loss:  0.030232971534132957
p mean is: tensor(-0.1727, device='cuda:4')
epoch:  44000 quantization_loss:  0.030180657282471657
p mean is: tensor(-0.1728, device='cuda:4')
epoch:  45000 quantization_loss:  0.030081164091825485
p mean is: tensor(-0.1721, device='cuda:4')
epoch:  46000 quantization_loss:  0.030046306550502777
p mean is: tensor(-0.1709, device='cuda:4')
epoch:  47000 quantization_loss:  0.03002077154815197
p mean is: tensor(-0.1706, device='cuda:4')
epoch:  48000 quantization_loss:  0.03000664710998535
p mean is: tensor(-0.1699, device='cuda:4')
epoch:  49000 quantization_loss:  0.029964009299874306
p mean is: tensor(-0.1689, device='cuda:4')
epoch:  50000 quantization_loss:  0.03001873753964901
p mean is: tensor(-0.1679, device='cuda:4')
epoch:  51000 quantization_loss:  0.02991352789103985
p mean is: tensor(-0.1674, device='cuda:4')
epoch:  52000 quantization_loss:  0.029928619042038918
p mean is: tensor(-0.1675, device='cuda:4')
epoch:  53000 quantization_loss:  0.029912829399108887
p mean is: tensor(-0.1670, device='cuda:4')
epoch:  54000 quantization_loss:  0.02987854555249214
p mean is: tensor(-0.1666, device='cuda:4')
epoch:  55000 quantization_loss:  0.029854705557227135
p mean is: tensor(-0.1666, device='cuda:4')
epoch:  56000 quantization_loss:  0.029875338077545166
p mean is: tensor(-0.1670, device='cuda:4')
epoch:  57000 quantization_loss:  0.02982720173895359
p mean is: tensor(-0.1668, device='cuda:4')
epoch:  58000 quantization_loss:  0.029849374666810036
p mean is: tensor(-0.1661, device='cuda:4')
epoch:  59000 quantization_loss:  0.029872510582208633
p mean is: tensor(-0.1654, device='cuda:4')
epoch:  60000 quantization_loss:  0.02984859049320221
p mean is: tensor(-0.1651, device='cuda:4')
epoch:  61000 quantization_loss:  0.029842784628272057
p mean is: tensor(-0.1645, device='cuda:4')
epoch:  62000 quantization_loss:  0.029803961515426636
p mean is: tensor(-0.1636, device='cuda:4')
epoch:  63000 quantization_loss:  0.02978050522506237
p mean is: tensor(-0.1633, device='cuda:4')
epoch:  64000 quantization_loss:  0.02977708913385868
p mean is: tensor(-0.1630, device='cuda:4')
epoch:  65000 quantization_loss:  0.029774842783808708
p mean is: tensor(-0.1631, device='cuda:4')
epoch:  66000 quantization_loss:  0.02985105663537979
p mean is: tensor(-0.1629, device='cuda:4')
epoch:  67000 quantization_loss:  0.02976752445101738
p mean is: tensor(-0.1638, device='cuda:4')
epoch:  68000 quantization_loss:  0.029772430658340454
p mean is: tensor(-0.1641, device='cuda:4')
epoch:  69000 quantization_loss:  0.029759764671325684
p mean is: tensor(-0.1640, device='cuda:4')
epoch:  70000 quantization_loss:  0.029752768576145172
p mean is: tensor(-0.1638, device='cuda:4')
epoch:  71000 quantization_loss:  0.029755478724837303
p mean is: tensor(-0.1635, device='cuda:4')
epoch:  72000 quantization_loss:  0.02987748384475708
p mean is: tensor(-0.1638, device='cuda:4')
epoch:  73000 quantization_loss:  0.029733125120401382
p mean is: tensor(-0.1637, device='cuda:4')
epoch:  74000 quantization_loss:  0.029757702723145485
p mean is: tensor(-0.1636, device='cuda:4')
epoch:  75000 quantization_loss:  0.029738858342170715
p mean is: tensor(-0.1634, device='cuda:4')
epoch:  76000 quantization_loss:  0.029731253162026405
p mean is: tensor(-0.1632, device='cuda:4')
epoch:  77000 quantization_loss:  0.029797585681080818
p mean is: tensor(-0.1629, device='cuda:4')
epoch:  78000 quantization_loss:  0.029732707887887955
p mean is: tensor(-0.1624, device='cuda:4')
epoch:  79000 quantization_loss:  0.029708445072174072
p mean is: tensor(-0.1622, device='cuda:4')
here
1.1.weight           | nonzeros =    6191 /   16384             ( 37.79%) | total_pruned =   10193 | shape = torch.Size([128, 128, 1, 1])
4.weight             | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
4.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
5.1.weight           | nonzeros =    4732 /   16384             ( 28.88%) | total_pruned =   11652 | shape = torch.Size([128, 128, 1, 1])
8.weight             | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
8.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
9.1.weight           | nonzeros =    6438 /   16384             ( 39.29%) | total_pruned =    9946 | shape = torch.Size([128, 128, 1, 1])
12.weight            | nonzeros =     108 /     128             ( 84.38%) | total_pruned =      20 | shape = torch.Size([128])
12.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
13.1.weight          | nonzeros =    5570 /   16384             ( 34.00%) | total_pruned =   10814 | shape = torch.Size([128, 128, 1, 1])
16.weight            | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
16.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
17.1.weight          | nonzeros =    2923 /   16384             ( 17.84%) | total_pruned =   13461 | shape = torch.Size([128, 128, 1, 1])
20.weight            | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
20.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
21.1.weight          | nonzeros =    1931 /   16384             ( 11.79%) | total_pruned =   14453 | shape = torch.Size([128, 128, 1, 1])
23.weight            | nonzeros =      77 /     128             ( 60.16%) | total_pruned =      51 | shape = torch.Size([128])
23.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
24.1.weight          | nonzeros =     130 /     384             ( 33.85%) | total_pruned =     254 | shape = torch.Size([3, 128, 1, 1])
alive: 28456, pruned : 71768, total: 100224, Compression rate :       3.52x  ( 71.61% pruned)
PSNR of output image is:  16.55947215347013
Experiment done
