(3, 512, 512)
Starting vanilla DIP on 6 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '20.131498265249302'
[128, 128, 128, 128, 128]
Number of params in decoder: 100224
torch.Size([1, 128, 16, 16])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/decoder_mask/6/SAM(sigma=0.1,lr=0.01,decay=0,beta=0)/det/-2.5/1e-09
epoch:  0 quantization_loss:  0.06551580876111984
p mean is: tensor(-0.0004, device='cuda:1')
epoch:  1000 quantization_loss:  0.062439318746328354
p mean is: tensor(-0.0192, device='cuda:1')
epoch:  2000 quantization_loss:  0.06124671921133995
p mean is: tensor(-0.0337, device='cuda:1')
epoch:  3000 quantization_loss:  0.06122157350182533
p mean is: tensor(-0.0484, device='cuda:1')
epoch:  4000 quantization_loss:  0.06125974655151367
p mean is: tensor(-0.0680, device='cuda:1')
epoch:  5000 quantization_loss:  0.05703257396817207
p mean is: tensor(-0.0866, device='cuda:1')
epoch:  6000 quantization_loss:  0.05299342796206474
p mean is: tensor(-0.1010, device='cuda:1')
epoch:  7000 quantization_loss:  0.05070871487259865
p mean is: tensor(-0.1188, device='cuda:1')
epoch:  8000 quantization_loss:  0.04903451353311539
p mean is: tensor(-0.1395, device='cuda:1')
epoch:  9000 quantization_loss:  0.04673805087804794
p mean is: tensor(-0.1632, device='cuda:1')
epoch:  10000 quantization_loss:  0.04410657659173012
p mean is: tensor(-0.1901, device='cuda:1')
epoch:  11000 quantization_loss:  0.04275742173194885
p mean is: tensor(-0.2222, device='cuda:1')
epoch:  12000 quantization_loss:  0.04103069007396698
p mean is: tensor(-0.2587, device='cuda:1')
epoch:  13000 quantization_loss:  0.03972867131233215
p mean is: tensor(-0.3017, device='cuda:1')
epoch:  14000 quantization_loss:  0.03928812965750694
p mean is: tensor(-0.3493, device='cuda:1')
epoch:  15000 quantization_loss:  0.0375920906662941
p mean is: tensor(-0.4009, device='cuda:1')
epoch:  16000 quantization_loss:  0.03698866814374924
p mean is: tensor(-0.4555, device='cuda:1')
epoch:  17000 quantization_loss:  0.03632058948278427
p mean is: tensor(-0.5113, device='cuda:1')
epoch:  18000 quantization_loss:  0.03586659952998161
p mean is: tensor(-0.5662, device='cuda:1')
epoch:  19000 quantization_loss:  0.0353463850915432
p mean is: tensor(-0.6196, device='cuda:1')
epoch:  20000 quantization_loss:  0.03501924127340317
p mean is: tensor(-0.6714, device='cuda:1')
epoch:  21000 quantization_loss:  0.034594811499118805
p mean is: tensor(-0.7209, device='cuda:1')
epoch:  22000 quantization_loss:  0.034274958074092865
p mean is: tensor(-0.7679, device='cuda:1')
epoch:  23000 quantization_loss:  0.033793434500694275
p mean is: tensor(-0.8130, device='cuda:1')
epoch:  24000 quantization_loss:  0.03331942856311798
p mean is: tensor(-0.8548, device='cuda:1')
epoch:  25000 quantization_loss:  0.033089034259319305
p mean is: tensor(-0.8949, device='cuda:1')
epoch:  26000 quantization_loss:  0.03283257782459259
p mean is: tensor(-0.9324, device='cuda:1')
epoch:  27000 quantization_loss:  0.03268912062048912
p mean is: tensor(-0.9672, device='cuda:1')
epoch:  28000 quantization_loss:  0.03251931816339493
p mean is: tensor(-0.9993, device='cuda:1')
epoch:  29000 quantization_loss:  0.032160475850105286
p mean is: tensor(-1.0292, device='cuda:1')
epoch:  30000 quantization_loss:  0.032196205109357834
p mean is: tensor(-1.0564, device='cuda:1')
epoch:  31000 quantization_loss:  0.03197088465094566
p mean is: tensor(-1.0816, device='cuda:1')
epoch:  32000 quantization_loss:  0.03194892406463623
p mean is: tensor(-1.1052, device='cuda:1')
epoch:  33000 quantization_loss:  0.03167610615491867
p mean is: tensor(-1.1268, device='cuda:1')
epoch:  34000 quantization_loss:  0.03158413618803024
p mean is: tensor(-1.1464, device='cuda:1')
epoch:  35000 quantization_loss:  0.03159554675221443
p mean is: tensor(-1.1646, device='cuda:1')
epoch:  36000 quantization_loss:  0.03151684254407883
p mean is: tensor(-1.1820, device='cuda:1')
epoch:  37000 quantization_loss:  0.03143942356109619
p mean is: tensor(-1.1980, device='cuda:1')
epoch:  38000 quantization_loss:  0.03140861541032791
p mean is: tensor(-1.2116, device='cuda:1')
epoch:  39000 quantization_loss:  0.03134600818157196
p mean is: tensor(-1.2248, device='cuda:1')
epoch:  40000 quantization_loss:  0.031412746757268906
p mean is: tensor(-1.2370, device='cuda:1')
epoch:  41000 quantization_loss:  0.031286273151636124
p mean is: tensor(-1.2481, device='cuda:1')
epoch:  42000 quantization_loss:  0.031249001622200012
p mean is: tensor(-1.2584, device='cuda:1')
epoch:  43000 quantization_loss:  0.03122187964618206
p mean is: tensor(-1.2669, device='cuda:1')
epoch:  44000 quantization_loss:  0.031186912208795547
p mean is: tensor(-1.2754, device='cuda:1')
epoch:  45000 quantization_loss:  0.031153200194239616
p mean is: tensor(-1.2837, device='cuda:1')
epoch:  46000 quantization_loss:  0.031133215874433517
p mean is: tensor(-1.2903, device='cuda:1')
epoch:  47000 quantization_loss:  0.031102150678634644
p mean is: tensor(-1.2964, device='cuda:1')
epoch:  48000 quantization_loss:  0.03093990683555603
p mean is: tensor(-1.3031, device='cuda:1')
epoch:  49000 quantization_loss:  0.030926967039704323
p mean is: tensor(-1.3087, device='cuda:1')
epoch:  50000 quantization_loss:  0.030902929604053497
p mean is: tensor(-1.3131, device='cuda:1')
epoch:  51000 quantization_loss:  0.03092588298022747
p mean is: tensor(-1.3176, device='cuda:1')
epoch:  52000 quantization_loss:  0.03090295009315014
p mean is: tensor(-1.3218, device='cuda:1')
epoch:  53000 quantization_loss:  0.03089073672890663
p mean is: tensor(-1.3258, device='cuda:1')
epoch:  54000 quantization_loss:  0.030880575999617577
p mean is: tensor(-1.3296, device='cuda:1')
epoch:  55000 quantization_loss:  0.030871471390128136
p mean is: tensor(-1.3331, device='cuda:1')
epoch:  56000 quantization_loss:  0.030851192772388458
p mean is: tensor(-1.3369, device='cuda:1')
epoch:  57000 quantization_loss:  0.030835075303912163
p mean is: tensor(-1.3395, device='cuda:1')
epoch:  58000 quantization_loss:  0.030833562836050987
p mean is: tensor(-1.3423, device='cuda:1')
epoch:  59000 quantization_loss:  0.030823448672890663
p mean is: tensor(-1.3454, device='cuda:1')
epoch:  60000 quantization_loss:  0.030843306332826614
p mean is: tensor(-1.3479, device='cuda:1')
epoch:  61000 quantization_loss:  0.03084191121160984
p mean is: tensor(-1.3505, device='cuda:1')
epoch:  62000 quantization_loss:  0.030875185504555702
p mean is: tensor(-1.3529, device='cuda:1')
epoch:  63000 quantization_loss:  0.0308277178555727
p mean is: tensor(-1.3551, device='cuda:1')
epoch:  64000 quantization_loss:  0.030733752995729446
p mean is: tensor(-1.3568, device='cuda:1')
epoch:  65000 quantization_loss:  0.030752763152122498
p mean is: tensor(-1.3590, device='cuda:1')
epoch:  66000 quantization_loss:  0.03073401376605034
p mean is: tensor(-1.3609, device='cuda:1')
epoch:  67000 quantization_loss:  0.03073187544941902
p mean is: tensor(-1.3630, device='cuda:1')
epoch:  68000 quantization_loss:  0.030799172818660736
p mean is: tensor(-1.3650, device='cuda:1')
epoch:  69000 quantization_loss:  0.030708618462085724
p mean is: tensor(-1.3666, device='cuda:1')
epoch:  70000 quantization_loss:  0.03070760704576969
p mean is: tensor(-1.3681, device='cuda:1')
epoch:  71000 quantization_loss:  0.03071526437997818
p mean is: tensor(-1.3694, device='cuda:1')
epoch:  72000 quantization_loss:  0.03069641999900341
p mean is: tensor(-1.3709, device='cuda:1')
epoch:  73000 quantization_loss:  0.030721785500645638
p mean is: tensor(-1.3722, device='cuda:1')
epoch:  74000 quantization_loss:  0.030695471912622452
p mean is: tensor(-1.3737, device='cuda:1')
epoch:  75000 quantization_loss:  0.030721470713615417
p mean is: tensor(-1.3751, device='cuda:1')
epoch:  76000 quantization_loss:  0.030681515112519264
p mean is: tensor(-1.3765, device='cuda:1')
epoch:  77000 quantization_loss:  0.030686315149068832
p mean is: tensor(-1.3777, device='cuda:1')
epoch:  78000 quantization_loss:  0.03069860115647316
p mean is: tensor(-1.3787, device='cuda:1')
epoch:  79000 quantization_loss:  0.030669592320919037
p mean is: tensor(-1.3796, device='cuda:1')
here
1.1.weight           | nonzeros =    4866 /   16384             ( 29.70%) | total_pruned =   11518 | shape = torch.Size([128, 128, 1, 1])
4.weight             | nonzeros =      73 /     128             ( 57.03%) | total_pruned =      55 | shape = torch.Size([128])
4.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
5.1.weight           | nonzeros =    3877 /   16384             ( 23.66%) | total_pruned =   12507 | shape = torch.Size([128, 128, 1, 1])
8.weight             | nonzeros =     104 /     128             ( 81.25%) | total_pruned =      24 | shape = torch.Size([128])
8.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
9.1.weight           | nonzeros =    5326 /   16384             ( 32.51%) | total_pruned =   11058 | shape = torch.Size([128, 128, 1, 1])
12.weight            | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
12.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
13.1.weight          | nonzeros =    3285 /   16384             ( 20.05%) | total_pruned =   13099 | shape = torch.Size([128, 128, 1, 1])
16.weight            | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
16.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
17.1.weight          | nonzeros =    2066 /   16384             ( 12.61%) | total_pruned =   14318 | shape = torch.Size([128, 128, 1, 1])
20.weight            | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
20.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
21.1.weight          | nonzeros =    3161 /   16384             ( 19.29%) | total_pruned =   13223 | shape = torch.Size([128, 128, 1, 1])
23.weight            | nonzeros =     112 /     128             ( 87.50%) | total_pruned =      16 | shape = torch.Size([128])
23.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
24.1.weight          | nonzeros =     174 /     384             ( 45.31%) | total_pruned =     210 | shape = torch.Size([3, 128, 1, 1])
alive: 23280, pruned : 76944, total: 100224, Compression rate :       4.31x  ( 76.77% pruned)
PSNR of output image is:  16.708558625086148
Experiment done
