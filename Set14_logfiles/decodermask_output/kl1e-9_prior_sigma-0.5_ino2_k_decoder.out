(3, 512, 512)
Starting vanilla DIP on 2 using SAM(sigma=0.1,lr=0.01,decay=0,beta=0)
Noisy PSNR is '20.427292317137017'
[128, 128, 128, 128, 128]
Number of params in decoder: 100224
torch.Size([1, 128, 16, 16])
Starting optimization with optimizer 'SAM'
Output directory: data/denoising/Set14/decoder_mask/2/SAM(sigma=0.1,lr=0.01,decay=0,beta=0)/det/-0.5/1e-09
epoch:  0 quantization_loss:  0.09514971822500229
p mean is: tensor(-6.6410e-05, device='cuda:1')
epoch:  1000 quantization_loss:  0.09222561120986938
p mean is: tensor(-0.0069, device='cuda:1')
epoch:  2000 quantization_loss:  0.08278725296258926
p mean is: tensor(-0.0083, device='cuda:1')
epoch:  3000 quantization_loss:  0.0677078515291214
p mean is: tensor(-0.0028, device='cuda:1')
epoch:  4000 quantization_loss:  0.06265754252672195
p mean is: tensor(-0.0009, device='cuda:1')
epoch:  5000 quantization_loss:  0.05784357711672783
p mean is: tensor(0.0008, device='cuda:1')
epoch:  6000 quantization_loss:  0.05229630321264267
p mean is: tensor(0.0038, device='cuda:1')
epoch:  7000 quantization_loss:  0.0466667115688324
p mean is: tensor(0.0030, device='cuda:1')
epoch:  8000 quantization_loss:  0.0416858084499836
p mean is: tensor(-0.0002, device='cuda:1')
epoch:  9000 quantization_loss:  0.03872116655111313
p mean is: tensor(-0.0037, device='cuda:1')
epoch:  10000 quantization_loss:  0.03607518598437309
p mean is: tensor(-0.0066, device='cuda:1')
epoch:  11000 quantization_loss:  0.03455900400876999
p mean is: tensor(-0.0098, device='cuda:1')
epoch:  12000 quantization_loss:  0.033624451607465744
p mean is: tensor(-0.0141, device='cuda:1')
epoch:  13000 quantization_loss:  0.03221092373132706
p mean is: tensor(-0.0194, device='cuda:1')
epoch:  14000 quantization_loss:  0.032004136592149734
p mean is: tensor(-0.0264, device='cuda:1')
epoch:  15000 quantization_loss:  0.030782688409090042
p mean is: tensor(-0.0329, device='cuda:1')
epoch:  16000 quantization_loss:  0.030440527945756912
p mean is: tensor(-0.0407, device='cuda:1')
epoch:  17000 quantization_loss:  0.030088650062680244
p mean is: tensor(-0.0499, device='cuda:1')
epoch:  18000 quantization_loss:  0.029559988528490067
p mean is: tensor(-0.0598, device='cuda:1')
epoch:  19000 quantization_loss:  0.028895044699311256
p mean is: tensor(-0.0698, device='cuda:1')
epoch:  20000 quantization_loss:  0.02843349426984787
p mean is: tensor(-0.0781, device='cuda:1')
epoch:  21000 quantization_loss:  0.027920998632907867
p mean is: tensor(-0.0862, device='cuda:1')
epoch:  22000 quantization_loss:  0.027641037479043007
p mean is: tensor(-0.0941, device='cuda:1')
epoch:  23000 quantization_loss:  0.027445219457149506
p mean is: tensor(-0.1015, device='cuda:1')
epoch:  24000 quantization_loss:  0.027266565710306168
p mean is: tensor(-0.1085, device='cuda:1')
epoch:  25000 quantization_loss:  0.027064641937613487
p mean is: tensor(-0.1159, device='cuda:1')
epoch:  26000 quantization_loss:  0.026883788406848907
p mean is: tensor(-0.1219, device='cuda:1')
epoch:  27000 quantization_loss:  0.026735534891486168
p mean is: tensor(-0.1263, device='cuda:1')
epoch:  28000 quantization_loss:  0.026532644405961037
p mean is: tensor(-0.1302, device='cuda:1')
epoch:  29000 quantization_loss:  0.026554277166724205
p mean is: tensor(-0.1345, device='cuda:1')
epoch:  30000 quantization_loss:  0.026266632601618767
p mean is: tensor(-0.1374, device='cuda:1')
epoch:  31000 quantization_loss:  0.02624347433447838
p mean is: tensor(-0.1404, device='cuda:1')
epoch:  32000 quantization_loss:  0.02613973617553711
p mean is: tensor(-0.1420, device='cuda:1')
epoch:  33000 quantization_loss:  0.026087645441293716
p mean is: tensor(-0.1430, device='cuda:1')
epoch:  34000 quantization_loss:  0.02625451236963272
p mean is: tensor(-0.1437, device='cuda:1')
epoch:  35000 quantization_loss:  0.025935007259249687
p mean is: tensor(-0.1445, device='cuda:1')
epoch:  36000 quantization_loss:  0.02591455541551113
p mean is: tensor(-0.1449, device='cuda:1')
epoch:  37000 quantization_loss:  0.025925740599632263
p mean is: tensor(-0.1452, device='cuda:1')
epoch:  38000 quantization_loss:  0.02601475454866886
p mean is: tensor(-0.1451, device='cuda:1')
epoch:  39000 quantization_loss:  0.025787752121686935
p mean is: tensor(-0.1446, device='cuda:1')
epoch:  40000 quantization_loss:  0.02577599138021469
p mean is: tensor(-0.1447, device='cuda:1')
epoch:  41000 quantization_loss:  0.025751223787665367
p mean is: tensor(-0.1450, device='cuda:1')
epoch:  42000 quantization_loss:  0.0257633775472641
p mean is: tensor(-0.1445, device='cuda:1')
epoch:  43000 quantization_loss:  0.025714149698615074
p mean is: tensor(-0.1446, device='cuda:1')
epoch:  44000 quantization_loss:  0.025708938017487526
p mean is: tensor(-0.1447, device='cuda:1')
epoch:  45000 quantization_loss:  0.02559485100209713
p mean is: tensor(-0.1441, device='cuda:1')
epoch:  46000 quantization_loss:  0.025560762733221054
p mean is: tensor(-0.1444, device='cuda:1')
epoch:  47000 quantization_loss:  0.02553483285009861
p mean is: tensor(-0.1448, device='cuda:1')
epoch:  48000 quantization_loss:  0.025577029213309288
p mean is: tensor(-0.1443, device='cuda:1')
epoch:  49000 quantization_loss:  0.02550535276532173
p mean is: tensor(-0.1439, device='cuda:1')
epoch:  50000 quantization_loss:  0.02550376020371914
p mean is: tensor(-0.1436, device='cuda:1')
epoch:  51000 quantization_loss:  0.025480110198259354
p mean is: tensor(-0.1434, device='cuda:1')
epoch:  52000 quantization_loss:  0.025497382506728172
p mean is: tensor(-0.1433, device='cuda:1')
epoch:  53000 quantization_loss:  0.02549947425723076
p mean is: tensor(-0.1437, device='cuda:1')
epoch:  54000 quantization_loss:  0.025455398485064507
p mean is: tensor(-0.1433, device='cuda:1')
epoch:  55000 quantization_loss:  0.025465719401836395
p mean is: tensor(-0.1421, device='cuda:1')
epoch:  56000 quantization_loss:  0.025561600923538208
p mean is: tensor(-0.1414, device='cuda:1')
epoch:  57000 quantization_loss:  0.02546244114637375
p mean is: tensor(-0.1413, device='cuda:1')
epoch:  58000 quantization_loss:  0.025431601330637932
p mean is: tensor(-0.1408, device='cuda:1')
epoch:  59000 quantization_loss:  0.025466345250606537
p mean is: tensor(-0.1397, device='cuda:1')
epoch:  60000 quantization_loss:  0.025430016219615936
p mean is: tensor(-0.1395, device='cuda:1')
epoch:  61000 quantization_loss:  0.025436146184802055
p mean is: tensor(-0.1396, device='cuda:1')
epoch:  62000 quantization_loss:  0.025432096794247627
p mean is: tensor(-0.1395, device='cuda:1')
epoch:  63000 quantization_loss:  0.025422606617212296
p mean is: tensor(-0.1389, device='cuda:1')
epoch:  64000 quantization_loss:  0.025417011231184006
p mean is: tensor(-0.1381, device='cuda:1')
epoch:  65000 quantization_loss:  0.025445958599448204
p mean is: tensor(-0.1376, device='cuda:1')
epoch:  66000 quantization_loss:  0.02542330138385296
p mean is: tensor(-0.1375, device='cuda:1')
epoch:  67000 quantization_loss:  0.025575529783964157
p mean is: tensor(-0.1374, device='cuda:1')
epoch:  68000 quantization_loss:  0.02542521432042122
p mean is: tensor(-0.1374, device='cuda:1')
epoch:  69000 quantization_loss:  0.025395741686224937
p mean is: tensor(-0.1378, device='cuda:1')
epoch:  70000 quantization_loss:  0.02538754977285862
p mean is: tensor(-0.1374, device='cuda:1')
epoch:  71000 quantization_loss:  0.02539638988673687
p mean is: tensor(-0.1375, device='cuda:1')
epoch:  72000 quantization_loss:  0.025392623618245125
p mean is: tensor(-0.1370, device='cuda:1')
epoch:  73000 quantization_loss:  0.025379983708262444
p mean is: tensor(-0.1367, device='cuda:1')
epoch:  74000 quantization_loss:  0.02541816607117653
p mean is: tensor(-0.1367, device='cuda:1')
epoch:  75000 quantization_loss:  0.02540832944214344
p mean is: tensor(-0.1366, device='cuda:1')
epoch:  76000 quantization_loss:  0.02537977509200573
p mean is: tensor(-0.1364, device='cuda:1')
epoch:  77000 quantization_loss:  0.02537671849131584
p mean is: tensor(-0.1363, device='cuda:1')
epoch:  78000 quantization_loss:  0.02538824826478958
p mean is: tensor(-0.1360, device='cuda:1')
epoch:  79000 quantization_loss:  0.025368988513946533
p mean is: tensor(-0.1355, device='cuda:1')
here
1.1.weight           | nonzeros =    6290 /   16384             ( 38.39%) | total_pruned =   10094 | shape = torch.Size([128, 128, 1, 1])
4.weight             | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
4.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
5.1.weight           | nonzeros =    5206 /   16384             ( 31.77%) | total_pruned =   11178 | shape = torch.Size([128, 128, 1, 1])
8.weight             | nonzeros =     121 /     128             ( 94.53%) | total_pruned =       7 | shape = torch.Size([128])
8.bias               | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
9.1.weight           | nonzeros =    7016 /   16384             ( 42.82%) | total_pruned =    9368 | shape = torch.Size([128, 128, 1, 1])
12.weight            | nonzeros =     111 /     128             ( 86.72%) | total_pruned =      17 | shape = torch.Size([128])
12.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
13.1.weight          | nonzeros =    5022 /   16384             ( 30.65%) | total_pruned =   11362 | shape = torch.Size([128, 128, 1, 1])
16.weight            | nonzeros =      90 /     128             ( 70.31%) | total_pruned =      38 | shape = torch.Size([128])
16.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
17.1.weight          | nonzeros =    1889 /   16384             ( 11.53%) | total_pruned =   14495 | shape = torch.Size([128, 128, 1, 1])
20.weight            | nonzeros =      48 /     128             ( 37.50%) | total_pruned =      80 | shape = torch.Size([128])
20.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
21.1.weight          | nonzeros =    2585 /   16384             ( 15.78%) | total_pruned =   13799 | shape = torch.Size([128, 128, 1, 1])
23.weight            | nonzeros =     122 /     128             ( 95.31%) | total_pruned =       6 | shape = torch.Size([128])
23.bias              | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
24.1.weight          | nonzeros =     242 /     384             ( 63.02%) | total_pruned =     142 | shape = torch.Size([3, 128, 1, 1])
alive: 28824, pruned : 71400, total: 100224, Compression rate :       3.48x  ( 71.24% pruned)
PSNR of output image is:  17.65782958787635
Experiment done
